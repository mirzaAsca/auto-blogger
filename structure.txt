â”œâ”€â”€ .DS_Store
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .github
â”‚   â”œâ”€â”€ dependabot.yml
â”‚   â””â”€â”€ workflows
â”‚       â””â”€â”€ docker-bulid.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .vscode
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ CODE_OF_CONDUCT.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ FOLDER
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ editor.py
â”‚   â”œâ”€â”€ gpt_researcher.py
â”‚   â”œâ”€â”€ html.py
â”‚   â”œâ”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ research_agent.py
â”‚   â”œâ”€â”€ research_team.py
â”‚   â”œâ”€â”€ researcher.py
â”‚   â”œâ”€â”€ reviser.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ search_api.py
â”‚   â”œâ”€â”€ singleton.py
â”‚   â”œâ”€â”€ test.py
â”‚   â”œâ”€â”€ text.py
â”‚   â”œâ”€â”€ web_scrape.py
â”‚   â”œâ”€â”€ web_search.py
â”‚   â””â”€â”€ writer.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Letter-to-USPTO-USCO-on-National-Commission-on-AI-1.pdf.crdownload
â”œâ”€â”€ National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf.crdownload
â”œâ”€â”€ README.md
â”œâ”€â”€ The Origin of LL.M. Programs_ A Case Study of the University of P.pdf.crdownload
â”œâ”€â”€ __pycache__
â”‚   â””â”€â”€ main.cpython-311.pyc
â”œâ”€â”€ actions
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ web_scrape.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ web_search.cpython-311.pyc
â”‚   â”œâ”€â”€ web_scrape.py
â”‚   â””â”€â”€ web_search.py
â”œâ”€â”€ agent
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ llm_utils.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ prompts.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ research_agent.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ run.cpython-311.pyc
â”‚   â”œâ”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ research_agent.py
â”‚   â””â”€â”€ run.py
â”œâ”€â”€ avip.pdf.crdownload
â”œâ”€â”€ client
â”‚   â”œâ”€â”€ .DS_Store
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ scripts.js
â”‚   â”œâ”€â”€ static
â”‚   â”‚   â”œâ”€â”€ academicResearchAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ businessAnalystAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ computerSecurityanalystAvatar.png
â”‚   â”‚   â”œâ”€â”€ defaultAgentAvatar2.JPG
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â”œâ”€â”€ financeAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ hulk.png
â”‚   â”‚   â”œâ”€â”€ mathAgentAvatar.png
â”‚   â”‚   â””â”€â”€ travelAgentAvatar.png
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ config.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ singleton.cpython-311.pyc
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ singleton.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ grain.pdf.crdownload
â”œâ”€â”€ js
â”‚   â””â”€â”€ overlay.js
â”œâ”€â”€ main.py
â”œâ”€â”€ p17.pdf.crdownload
â”œâ”€â”€ permchain_example
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ editor_actors
â”‚   â”‚   â””â”€â”€ editor.py
â”‚   â”œâ”€â”€ research_team.py
â”‚   â”œâ”€â”€ researcher.py
â”‚   â”œâ”€â”€ reviser_actors
â”‚   â”‚   â””â”€â”€ reviser.py
â”‚   â”œâ”€â”€ search_actors
â”‚   â”‚   â”œâ”€â”€ gpt_researcher.py
â”‚   â”‚   â””â”€â”€ search_api.py
â”‚   â”œâ”€â”€ test.py
â”‚   â””â”€â”€ writer_actors
â”‚       â””â”€â”€ writer.py
â”œâ”€â”€ processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ html.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ text.cpython-311.pyc
â”‚   â”œâ”€â”€ html.py
â”‚   â””â”€â”€ text.py
â”œâ”€â”€ prompts.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ structure.py
â””â”€â”€ structure.txt

==============================

-settings.json
{
  "[python]": {
    "editor.defaultFormatter": "ms-python.black-formatter"
  },
  "python.formatting.provider": "none"
}


==============================

-config.py
"""Configuration class to store the state of bools for different scripts access."""
import os

import openai
from colorama import Fore
from dotenv import load_dotenv

from config.singleton import Singleton

load_dotenv(verbose=True)


class Config(metaclass=Singleton):
    """
    Configuration class to store the state of bools for different scripts access.
    """

    def __init__(self) -> None:
        """Initialize the Config class"""
        self.debug_mode = False
        self.allow_downloads = False

        self.selenium_web_browser = os.getenv("USE_WEB_BROWSER", "chrome")
        self.llm_provider = os.getenv("LLM_PROVIDER", "ChatOpenAI")
        self.fast_llm_model = os.getenv("FAST_LLM_MODEL", "gpt-3.5-turbo-16k")
        self.smart_llm_model = os.getenv("SMART_LLM_MODEL", "gpt-4")
        self.fast_token_limit = int(os.getenv("FAST_TOKEN_LIMIT", 2000))
        self.smart_token_limit = int(os.getenv("SMART_TOKEN_LIMIT", 4000))
        self.browse_chunk_max_length = int(os.getenv("BROWSE_CHUNK_MAX_LENGTH", 8192))
        self.summary_token_limit = int(os.getenv("SUMMARY_TOKEN_LIMIT", 700))

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.temperature = float(os.getenv("TEMPERATURE", "1"))

        self.user_agent = os.getenv(
            "USER_AGENT",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36"
            " (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36",
        )

        self.memory_backend = os.getenv("MEMORY_BACKEND", "local")
        # Initialize the OpenAI API client
        openai.api_key = self.openai_api_key

    def set_fast_llm_model(self, value: str) -> None:
        """Set the fast LLM model value."""
        self.fast_llm_model = value

    def set_smart_llm_model(self, value: str) -> None:
        """Set the smart LLM model value."""
        self.smart_llm_model = value

    def set_fast_token_limit(self, value: int) -> None:
        """Set the fast token limit value."""
        self.fast_token_limit = value

    def set_smart_token_limit(self, value: int) -> None:
        """Set the smart token limit value."""
        self.smart_token_limit = value

    def set_browse_chunk_max_length(self, value: int) -> None:
        """Set the browse_website command chunk max length value."""
        self.browse_chunk_max_length = value

    def set_openai_api_key(self, value: str) -> None:
        """Set the OpenAI API key value."""
        self.openai_api_key = value

    def set_debug_mode(self, value: bool) -> None:
        """Set the debug mode value."""
        self.debug_mode = value


def check_openai_api_key() -> None:
    """Check if the OpenAI API key is set in config.py or as an environment variable."""
    cfg = Config()
    if not cfg.openai_api_key:
        print(
            Fore.RED
            + "Please set your OpenAI API key in .env or as an environment variable."
        )
        print("You can get your key from https://platform.openai.com/account/api-keys")
        exit(1)


==============================

-editor.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

EDIT_TEMPLATE = """You are an editor. \
You have been tasked with editing the following draft, which was written by a non-expert. \
Please accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision. \
Things you should be checking for:

- This draft MUST fully answer the original question
- This draft MUST be written in apa format

If not all of the above criteria are met, you should send appropriate revision notes.
"""


class EditorActor:
    def __init__(self):
        print("EDITOR Initializing EditorActor")  # Indicates the start of the EditorActor initialization

        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(EDIT_TEMPLATE) + "Draft:\n\n{draft}"
        self.functions = [
            {
                "name": "revise",
                "description": "Sends the draft for revision",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "notes": {
                            "type": "string",
                            "description": "The editor's notes to guide the revision.",
                        },
                    },
                },
            },
            {
                "name": "accept",
                "description": "Accepts the draft",
                "parameters": {
                    "type": "object",
                    "properties": {"ready": {"const": True}},
                },
            },
        ]

    @property
    def runnable(self):
        return (
            self.prompt | self.model.bind(functions=self.functions)
        )


==============================

-gpt_researcher.py
import json
# Import necessary modules for text processing, web scraping, and searching
from processing.text import summarize_text
from actions.web_scrape import scrape_text_with_selenium
from actions.web_search import web_search

# Import Langchain and related utilities for AI-driven chat and prompt management
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableMap, RunnableLambda
from langchain.schema.messages import SystemMessage
from agent.prompts import auto_agent_instructions, generate_search_queries_prompt
from config import Config

# Load global configurations
CFG = Config()

# [Initiation] Prepare the AI-driven message template for generating search queries
search_message = (generate_search_queries_prompt("{question}"))
SEARCH_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "{agent_prompt}"),
    ("user", search_message)
])

# Load instructions for automated agent behavior
AUTO_AGENT_INSTRUCTIONS = auto_agent_instructions()
CHOOSE_AGENT_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessage(content=AUTO_AGENT_INSTRUCTIONS),
    ("user", "task: {task}")
])

# [Content Retrieval and Summarization and Analysis] Define the process for scraping and summarizing text from a URL
scrape_and_summarize = {
    "question": lambda x: x["question"],
    "text": lambda x: scrape_text_with_selenium(x['url'])[1],
    "url": lambda x: x['url']
} | RunnableMap({
        "summary": lambda x: summarize_text(text=x["text"], question=x["question"], url=x["url"]),
        "url": lambda x: x['url']
}) | (lambda x: f"Source Url: {x['url']}\nSummary: {x['summary']}")

# Initialize a set to keep track of URLs that have already been seen to avoid duplicate content
seen_urls = set()

# [Web Search and Content Retrieval] Define the process for conducting multiple searches, avoiding duplicate URLs, and processing the results
multi_search = (
    lambda x: [
        {"url": url.get("href"), "question": x["question"]}
        for url in json.loads(web_search(query=x["question"], num_results=3))
        if not (url.get("href") in seen_urls or seen_urls.add(url.get("href")))
   ]
) | scrape_and_summarize.map() | (lambda x: "\n".join(x))

# Set up the search query and agent choice mechanisms using AI models
search_query = SEARCH_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads
choose_agent = CHOOSE_AGENT_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads

# [Initiation] Define how to get search queries based on agent prompts
get_search_queries = {
    "question": lambda x: x,
    "agent_prompt": {"task": lambda x: x} | choose_agent | (lambda x: x["agent_role_prompt"])
} | search_query


class GPTResearcherActor:
    # [Compilation and Output] Define the complete runnable process for the GPT Researcher, compiling all steps
    @property
    def runnable(self):
        return (
            get_search_queries
            | (lambda x: [{"question": q} for q in x])
            | multi_search.map()
            | (lambda x: "\n\n".join(x))
        )


==============================

-html.py
"""HTML processing functions"""
from __future__ import annotations

from bs4 import BeautifulSoup
from requests.compat import urljoin


def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """
    return [
        (link.text, urljoin(base_url, link["href"]))
        for link in soup.find_all("a", href=True)
    ]


def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """
    return [f"{link_text} ({link_url})" for link_text, link_url in hyperlinks]


==============================

-llm_utils.py
from __future__ import annotations

import json

from fastapi import WebSocket
import time

import openai
from langchain.adapters import openai as lc_openai
from colorama import Fore, Style
from openai.error import APIError, RateLimitError

from agent.prompts import auto_agent_instructions
from config import Config

CFG = Config()

openai.api_key = CFG.openai_api_key

from typing import Optional
import logging


def create_chat_completion(
    messages: list,  # type: ignore
    model: Optional[str] = None,
    temperature: float = CFG.temperature,
    max_tokens: Optional[int] = None,
    stream: Optional[bool] = False,
    websocket: WebSocket | None = None,
) -> str:
    """Create a chat completion using the OpenAI API
    Args:
        messages (list[dict[str, str]]): The messages to send to the chat completion
        model (str, optional): The model to use. Defaults to None.
        temperature (float, optional): The temperature to use. Defaults to 0.9.
        max_tokens (int, optional): The max tokens to use. Defaults to None.
        stream (bool, optional): Whether to stream the response. Defaults to False.
    Returns:
        str: The response from the chat completion
    """

    # validate input
    if model is None:
        raise ValueError("Model cannot be None")
    if max_tokens is not None and max_tokens > 8001:
        raise ValueError(f"Max tokens cannot be more than 8001, but got {max_tokens}")
    if stream and websocket is None:
        raise ValueError("Websocket cannot be None when stream is True")

    # create response
    for attempt in range(10):  # maximum of 10 attempts
        response = send_chat_completion_request(
            messages, model, temperature, max_tokens, stream, websocket
        )
        return response

    logging.error("Failed to get response from OpenAI API")
    raise RuntimeError("Failed to get response from OpenAI API")


def send_chat_completion_request(
    messages, model, temperature, max_tokens, stream, websocket
):
    if not stream:
        result = lc_openai.ChatCompletion.create(
            model=model,  # Change model here to use different models
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            provider=CFG.llm_provider,  # Change provider here to use a different API
        )
        return result["choices"][0]["message"]["content"]
    else:
        return stream_response(model, messages, temperature, max_tokens, websocket)


async def stream_response(model, messages, temperature, max_tokens, websocket):
    paragraph = ""
    response = ""
    print(f"LLM_UTILS streaming response...")

    for chunk in lc_openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        provider=CFG.llm_provider,
        stream=True,
    ):
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content is not None:
            response += content
            paragraph += content
            if "\n" in paragraph:
                await websocket.send_json({"type": "report", "output": paragraph})
                paragraph = ""
    print(f"LLM_UTILS streaming response complete")
    return response


def choose_agent(task: str) -> dict:
    """Determines what agent should be used
    Args:
        task (str): The research question the user asked
    Returns:
        agent - The agent that will be used
        agent_role_prompt (str): The prompt for the agent
    """
    try:
        response = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{auto_agent_instructions()}"},
                {"role": "user", "content": f"task: {task}"},
            ],
            temperature=0,
        )

        return json.loads(response)
    except Exception as e:
        print(f"{Fore.RED}Error in choose_agent: {e}{Style.RESET_ALL}")
        return {
            "agent": "Default Agent",
            "agent_role_prompt": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
        }


==============================

-main.py
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import os

from agent.llm_utils import choose_agent
from agent.run import WebSocketManager


class ResearchRequest(BaseModel):
    task: str
    report_type: str
    agent: str



app = FastAPI()
app.mount("/site", StaticFiles(directory="client"), name="site")
app.mount("/static", StaticFiles(directory="client/static"), name="static")
# Dynamic directory for outputs once first research is run
@app.on_event("startup")
def startup_event():
    if not os.path.isdir("outputs"):
        os.makedirs("outputs")
    app.mount("/outputs", StaticFiles(directory="outputs"), name="outputs")

templates = Jinja2Templates(directory="client")

manager = WebSocketManager()


@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse('index.html', {"request": request, "report": None})


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            if data.startswith("start"):
                json_data = json.loads(data[6:])
                task = json_data.get("task")
                report_type = json_data.get("report_type")
                agent = json_data.get("agent")
                # temporary so "normal agents" can still be used and not just auto generated, will be removed when we move to auto generated
                if agent == "Auto Agent":
                    agent_dict = choose_agent(task)
                    agent = agent_dict.get("agent")
                    agent_role_prompt = agent_dict.get("agent_role_prompt")
                else:
                    agent_role_prompt = None

                await websocket.send_json({"type": "logs", "output": f"Initiated an Agent: {agent}"})
                if task and report_type and agent:
                    await manager.start_streaming(task, report_type, agent, agent_role_prompt, websocket)
                else:
                    print("Error: not enough parameters provided.")

    except WebSocketDisconnect:
        await manager.disconnect(websocket)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)


==============================

-prompts.py
from datetime import datetime


def generate_agent_role_prompt(agent):
    """Generates the agent role prompt.
    Args: agent (str): The type of the agent.
    Returns: str: The agent role prompt.
    """
    prompts = {
        "Finance Agent": "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.",
        "Travel Agent": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.",
        "Academic Research Agent": "You are an AI academic research assistant. Your primary responsibility is to create thorough, academically rigorous, unbiased, and systematically organized reports on a given research topic, following the standards of scholarly work.",
        "Business Analyst": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.",
        "Computer Security Analyst Agent": "You are an AI specializing in computer security analysis. Your principal duty is to generate comprehensive, meticulously detailed, impartial, and systematically structured reports on computer security topics. This includes Exploits, Techniques, Threat Actors, and Advanced Persistent Threat (APT) Groups. All produced reports should adhere to the highest standards of scholarly work and provide in-depth insights into the complexities of computer security.",
        "Default Agent": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
    }

    return prompts.get(agent, "No such agent")


def generate_report_prompt(question, research_summary):
    """Generates the report prompt for the given question and research summary.
    Args: question (str): The question to generate the report prompt for
            research_summary (str): The research summary to generate the report prompt for
    Returns: str: The report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, answer the following'
        f' question or topic: "{question}" in a detailed report --'
        " The report should focus on the answer to the question, should be well structured, informative,"
        " in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format.\n "
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n"
        f"Write all used source urls at the end of the report in apa format.\n "
        f"Assume that the current date is {datetime.now().strftime('%B %d, %Y')}"
    )


def generate_search_queries_prompt(question):
    """Generates the search queries prompt for the given question.
    Args: question (str): The question to generate the search queries prompt for
    Returns: str: The search queries prompt for the given question
    """
    print(
        f"PROMPTS Generating search queries prompt for: {question}"
    )  # Indicates the specific question being processed for search queries

    queries = (
        f'Write 3 google search queries to search online that form an objective opinion from the following: "{question}"'
        f'Use the current date if needed: {datetime.now().strftime("%B %d, %Y")}.\n'
        f'You must respond with a list of strings in the following format: ["query 1", "query 2", "query 3"].'
    )
    print(f"PROMPTS Generated search queries: {queries}")  # Print the queries

    return queries


def generate_resource_report_prompt(question, research_summary):
    """Generates the resource report prompt for the given question and research summary.

    Args:
        question (str): The question to generate the resource report prompt for.
        research_summary (str): The research summary to generate the resource report prompt for.

    Returns:
        str: The resource report prompt for the given question and research summary.
    """
    return (
        f'"""{research_summary}""" Based on the above information, generate a bibliography recommendation report for the following'
        f' question or topic: "{question}". The report should provide a detailed analysis of each recommended resource,'
        " explaining how each source can contribute to finding answers to the research question."
        " Focus on the relevance, reliability, and significance of each source."
        " Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax."
        " Include relevant facts, figures, and numbers whenever available."
        " The report should have a minimum length of 1,200 words."
    )


def generate_outline_report_prompt(question, research_summary):
    """Generates the outline report prompt for the given question and research summary.
    Args: question (str): The question to generate the outline report prompt for
            research_summary (str): The research summary to generate the outline report prompt for
    Returns: str: The outline report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, generate an outline for a research report in Markdown syntax'
        f' for the following question or topic: "{question}". The outline should provide a well-structured framework'
        " for the research report, including the main sections, subsections, and key points to be covered."
        " The research report should be detailed, informative, in-depth, and a minimum of 1,200 words."
        " Use appropriate Markdown syntax to format the outline and ensure readability."
    )


def generate_concepts_prompt(question, research_summary):
    """Generates the concepts prompt for the given question.
    Args: question (str): The question to generate the concepts prompt for
            research_summary (str): The research summary to generate the concepts prompt for
    Returns: str: The concepts prompt for the given question
    """

    return (
        f'"""{research_summary}""" Using the above information, generate a list of 5 main concepts to learn for a research report'
        f' on the following question or topic: "{question}". The outline should provide a well-structured framework'
        'You must respond with a list of strings in the following format: ["concepts 1", "concepts 2", "concepts 3", "concepts 4, concepts 5"]'
    )


def generate_lesson_prompt(concept):
    """
    Generates the lesson prompt for the given question.
    Args:
        concept (str): The concept to generate the lesson prompt for.
    Returns:
        str: The lesson prompt for the given concept.
    """

    prompt = (
        f"generate a comprehensive lesson about {concept} in Markdown syntax. This should include the definition"
        f"of {concept}, its historical background and development, its applications or uses in different"
        f"fields, and notable events or facts related to {concept}."
    )

    return prompt


def get_report_by_type(report_type):
    report_type_mapping = {
        "research_report": generate_report_prompt,
        "resource_report": generate_resource_report_prompt,
        "outline_report": generate_outline_report_prompt,
    }
    return report_type_mapping[report_type]


def auto_agent_instructions():
    return """
        This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific agent, defined by its type and role, with each agent requiring distinct instructions.
        Agent
        The agent is determined by the field of the topic and the specific name of the agent that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each agent type is associated with a corresponding emoji.

        examples:
        task: "should I invest in apple stocks?"
        response: 
        {
            "agent": "ðŸ’° Finance Agent",
            "agent_role_prompt: "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends."
        }
        task: "could reselling sneakers become profitable?"
        response: 
        { 
            "agent":  "ðŸ“ˆ Business Analyst Agent",
            "agent_role_prompt": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis."
        }
        task: "what are the most interesting sites in Tel Aviv?"
        response:
        {
            "agent:  "ðŸŒ Travel Agent",
            "agent_role_prompt": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights."
        }
    """


==============================

-research_agent.py
# Description: Research assistant class that handles the research process for a given question.

# libraries
import asyncio
import json
import hashlib
from actions.web_search import web_search
from actions.web_scrape import async_browse
from processing.text import (
    write_to_file,
    create_message,
    create_chat_completion,
    read_txt_files,
    write_md_to_pdf,
)
from config import Config
from agent import prompts
import os
import string

CFG = Config()


class ResearchAgent:
    def __init__(self, question, agent, agent_role_prompt, websocket=None):
        """Initializes the research assistant with the given question.
        Args: question (str): The question to research
        Returns: None
        """
        self.question = question
        self.agent = agent
        self.agent_role_prompt = (
            agent_role_prompt
            if agent_role_prompt
            else prompts.generate_agent_role_prompt(agent)
        )
        self.visited_urls = set()
        self.research_summary = ""
        self.dir_path = f"./outputs/{hashlib.sha1(question.encode()).hexdigest()}"
        self.websocket = websocket

        print("RESEARCH_AGENT ResearchAgent initialized with question:", self.question)

    async def stream_output(self, output):
        if not self.websocket:
            return print(output)
        await self.websocket.send_json({"type": "logs", "output": output})

    async def summarize(self, text, topic):
        """Summarizes the given text for the given topic.
        Args: text (str): The text to summarize
                topic (str): The topic to summarize the text for
        Returns: str: The summarized text
        """
        print("RESEARCH_AGENT Starting summarization for topic:", topic)

        messages = [create_message(text, topic)]
        await self.stream_output(f"ðŸ“ Summarizing text for query: {text}")

        return create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
        )

    async def get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """
        new_urls = []
        for url in url_set_input:
            if url not in self.visited_urls:
                await self.stream_output(f"âœ… Adding source url to research: {url}\n")

                self.visited_urls.add(url)
                new_urls.append(url)

        return new_urls

    async def call_agent(self, action, stream=False, websocket=None):
        messages = [
            {"role": "system", "content": self.agent_role_prompt},
            {
                "role": "user",
                "content": action,
            },
        ]
        answer = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=messages,
            stream=stream,
            websocket=websocket,
        )
        return answer

    async def create_search_queries(self):
        """Modifies the method to use the user's input directly as the search query.
        Args: None
        Returns: list[str]: The search queries for the given question
        """
        # Directly use the user's input (self.question) as the search query
        await self.stream_output(
            f"ðŸ§  I will conduct my research based on the following query: '{self.question}'..."
        )
        return [self.question]

    async def async_search(self, query):
        """Runs the async search for the given query.
        Args: query (str): The query to run the async search for
        Returns: list[str]: The async search for the given query
        """
        search_results = json.loads(web_search(query))
        new_search_urls = await self.get_new_urls([url.get("link") for url in search_results])

        await self.stream_output(
            f"ðŸŒ Browsing the following sites for relevant information: {new_search_urls}..."
        )

        # Create a list to hold the coroutine objects
        tasks = [
            async_browse(url, query, self.websocket) for url in new_search_urls
        ]

        # Gather the results as they become available
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        return responses

    async def run_search_summary(self, query):
        """Runs the search summary for the given query.
        Args: query (str): The query to run the search summary for
        Returns: str: The search summary for the given query
        """
        await self.stream_output(f"ðŸ”Ž Running research for '{query}'...")

        responses = await self.async_search(query)

        result = "\n".join(responses)
        os.makedirs(
            os.path.dirname(f"{self.dir_path}/research-{query}.txt"), exist_ok=True
        )
        write_to_file(f"{self.dir_path}/research-{query}.txt", result)
        return result

    async def conduct_research(self):
        """Conducts the research for the given question.
        Args: None
        Returns: str: The research for the given question
        """
        self.research_summary = (
            read_txt_files(self.dir_path) if os.path.isdir(self.dir_path) else ""
        )

        if not self.research_summary:
            search_queries = await self.create_search_queries()
            for query in search_queries:
                research_result = await self.run_search_summary(query)
                self.research_summary += f"{research_result}\n\n"

        await self.stream_output(
            f"Total research words: {len(self.research_summary.split(' '))}"
        )

        return self.research_summary

    async def create_concepts(self):
        """Creates the concepts for the given question.
        Args: None
        Returns: list[str]: The concepts for the given question
        """
        result = await self.call_agent(
            prompts.generate_concepts_prompt(self.question, self.research_summary)
        )

        await self.stream_output(
            f"I will research based on the following concepts: {result}\n"
        )
        return json.loads(result)

    async def write_report(self, report_type, websocket=None):
        """Writes the report for the given question.
        Args: None
        Returns: str: The report for the given question
        """
        print("RESEARCH_AGENT Starting to write report of type:", report_type)

        report_type_func = prompts.get_report_by_type(report_type)
        await self.stream_output(
            f"âœï¸ Writing {report_type} for research task: {self.question}..."
        )

        answer = await self.call_agent(
            report_type_func(self.question, self.research_summary),
            stream=websocket is not None,
            websocket=websocket,
        )
        # if websocket is True than we are streaming gpt response, so we need to wait for the final response
        final_report = await answer if websocket else answer

        path = await write_md_to_pdf(report_type, self.dir_path, final_report)

        return answer, path

    async def write_lessons(self):
        """Writes lessons on essential concepts of the research.
        Args: None
        Returns: None
        """
        concepts = await self.create_concepts()
        for concept in concepts:
            answer = await self.call_agent(
                prompts.generate_lesson_prompt(concept), stream=True
            )
            await write_md_to_pdf("Lesson", self.dir_path, answer)


==============================

-research_team.py
from operator import itemgetter
from langchain.runnables.openai_functions import OpenAIFunctionsRouter

from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic

"""
    This is the research team. 
    It is a group of autonomous agents that work together to answer a given question
    using a comprehensive research process that includes: 
        - Searching for relevant information across multiple sources
        - Extracting relevant information
        - Writing a well structured report
        - Validating the report
        - Revising the report
        - Repeat until the report is satisfactory
"""


class ResearchTeam:
    def __init__(self, research_actor, editor_actor, reviser_actor):
        self.research_actor_instance = research_actor
        self.editor_actor_instance = editor_actor
        self.revise_actor_instance = reviser_actor

    def run(self, query):
        print("ResearchTeam: Initialized with query:", query)  # Starting the research process

        # create topics
        editor_inbox = Topic("editor_inbox")
        reviser_inbox = Topic("reviser_inbox")
        print("ResearchTeam: Topics created - editor_inbox, reviser_inbox")

        research_chain = (
            # Listed in inputs
            Topic.IN.subscribe()
            | {"draft": lambda x: self.research_actor_instance.run(x["question"])}
            # The draft always goes to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Research chain initialized")

        editor_chain = (
            # Listen for events in the editor_inbox
            editor_inbox.subscribe()
            | self.editor_actor_instance.runnable
            # Depending on the output, different things should happen
            | OpenAIFunctionsRouter(
                {
                    # If revise is chosen, we send a push to the critique_inbox
                    "revise": (
                        {
                            "notes": itemgetter("notes"),
                            "draft": editor_inbox.current() | itemgetter("draft"),
                            "question": Topic.IN.current() | itemgetter("question"),
                        }
                        | reviser_inbox.publish()
                    ),
                    # If accepted, then we return
                    "accept": editor_inbox.current() | Topic.OUT.publish(),
                }
            )
        )
        print("ResearchTeam: Editor chain initialized")

        reviser_chain = (
            # Listen for events in the reviser's inbox
            reviser_inbox.subscribe()
            | self.revise_actor_instance.runnable
            # Publish to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Reviser chain initialized")

        web_researcher = PubSub(
            research_chain,
            editor_chain,
            reviser_chain,
            connection=InMemoryPubSubConnection(),
        )
        print("ResearchTeam: PubSub initialized with all chains")

        res = web_researcher.invoke({"question": query})
        print("ResearchTeam: Invocation complete with result:", res)

        return res["draft"]


==============================

-researcher.py
from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic


class Researcher:
    def __init__(self, search_actor, writer_actor):
        print("RESEARCHER Initializing Researcher...")
        self.search_actor_instance = search_actor
        self.writer_actor_instance = writer_actor

    def run(self, query):
        # The research inbox
        research_inbox = Topic("research")
        search_actor = (
            Topic.IN.subscribe()
            | {"query": lambda x: x, "results": self.search_actor_instance.runnable}
            | research_inbox.publish()
        )

        write_actor = (
            research_inbox.subscribe()
            | self.writer_actor_instance.runnable
            | Topic.OUT.publish()
        )

        researcher = PubSub(
            search_actor,
            write_actor,
            connection=InMemoryPubSubConnection(),
        )

        res = researcher.invoke(query)
        return res["answer"]


==============================

-reviser.py
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

class ReviserActor:
    def __init__(self):
        print("REVISER Initializing ReviserActor")  # Indicates the start of the EditorActor initialization

        
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(
            "You are an expert writer. "
            "You have been tasked by your editor with revising the following draft, which was written by a non-expert. "
            "You may follow the editor's notes or not, as you see fit."
        ) + "Draft:\n\n{draft}" + "Editor's notes:\n\n{notes}"

    @property
    def runnable(self):
        return {
            "draft": {
                "draft": lambda x: x["draft"],
                "notes": lambda x: x["notes"],
            } | self.prompt | self.model | StrOutputParser()
        }


==============================

-run.py
#manage WebSocket connections and the execution of an agent's research task
import asyncio
import datetime

from typing import List, Dict
from fastapi import WebSocket
from config import check_openai_api_key
from agent.research_agent import ResearchAgent


class WebSocketManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}
        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}

    async def start_sender(self, websocket: WebSocket):
        queue = self.message_queues[websocket]
        while True:
            message = await queue.get()
            if websocket in self.active_connections:
                await websocket.send_text(message)
            else:
                break

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.message_queues[websocket] = asyncio.Queue()
        self.sender_tasks[websocket] = asyncio.create_task(self.start_sender(websocket))

    async def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        self.sender_tasks[websocket].cancel()
        del self.sender_tasks[websocket]
        del self.message_queues[websocket]

    async def start_streaming(self, task, report_type, agent, agent_role_prompt, websocket):
        report, path = await run_agent(task, report_type, agent, agent_role_prompt, websocket)
        return report, path


async def run_agent(task, report_type, agent, agent_role_prompt, websocket):
    check_openai_api_key()

    start_time = datetime.datetime.now()

    # await websocket.send_json({"type": "logs", "output": f"Start time: {str(start_time)}\n\n"})

    assistant = ResearchAgent(task, agent, agent_role_prompt, websocket)
    await assistant.conduct_research()

    report, path = await assistant.write_report(report_type, websocket)

    await websocket.send_json({"type": "path", "output": path})

    end_time = datetime.datetime.now()
    await websocket.send_json({"type": "logs", "output": f"\nEnd time: {end_time}\n"})
    await websocket.send_json({"type": "logs", "output": f"\nTotal run time: {end_time - start_time}\n"})

    return report, path


==============================

-search_api.py
from tavily import Client
import os
from langchain.schema.runnable import RunnableLambda


class TavilySearchActor:
    def __init__(self):
        self.api_key = os.environ["TAVILY_API_KEY"]

    @property
    def runnable(self):
        client = Client(self.api_key)
        return RunnableLambda(client.advanced_search) | {"results": lambda x: x["results"]}


==============================

-singleton.py
"""The singleton metaclass for ensuring only one instance of a class."""
import abc


class Singleton(abc.ABCMeta, type):
    """
    Singleton metaclass for ensuring only one instance of a class.
    """

    _instances = {}

    def __call__(cls, *args, **kwargs):
        """Call method for the singleton metaclass."""
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class AbstractSingleton(abc.ABC, metaclass=Singleton):
    """
    Abstract singleton class for ensuring only one instance of a class.
    """

    pass


==============================

-test.py
# main
import os, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from permchain_example.researcher import Researcher
from permchain_example.search_actors.search_api import TavilySearchActor
from permchain_example.editor_actors.editor import EditorActor
from permchain_example.reviser_actors.reviser import ReviserActor
from permchain_example.search_actors.gpt_researcher import GPTResearcherActor
from permchain_example.writer_actors.writer import WriterActor
from permchain_example.research_team import ResearchTeam
from processing.text import md_to_pdf



if __name__ == '__main__':
    output_path = "./output"
    if not os.path.exists(output_path):
        # If the directory does not exist, create it
        os.makedirs(output_path)

    stocks = ["NVDA"]

    for stock in stocks[:1]:
        query = f"is the stock {stock} a good buy?"
        researcher = Researcher(GPTResearcherActor(), WriterActor())
        research_team = ResearchTeam(researcher, EditorActor(), ReviserActor())

        draft = research_team.run(query)
        with open(f"{output_path}/{stock}.md", "w") as f:
            f.write(draft)
        md_to_pdf(f"{output_path}/{stock}.md", f"{output_path}/{stock}.pdf")

==============================

-text.py
"""Text processing functions"""
import urllib
from typing import Dict, Generator, Optional
import string

from selenium.webdriver.remote.webdriver import WebDriver

from config import Config
from agent.llm_utils import create_chat_completion
import os
from md2pdf.core import md2pdf

CFG = Config()


def split_text(text: str, max_length: int = 8192) -> Generator[str, None, None]:
    """Split text into chunks of a maximum length

    Args:
        text (str): The text to split
        max_length (int, optional): The maximum length of each chunk. Defaults to 8192.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: If the text is longer than the maximum length
    """
    paragraphs = text.split("\n")
    current_length = 0
    current_chunk = []

    for paragraph in paragraphs:
        if current_length + len(paragraph) + 1 <= max_length:
            current_chunk.append(paragraph)
            current_length += len(paragraph) + 1
        else:
            yield "\n".join(current_chunk)
            current_chunk = [paragraph]
            current_length = len(paragraph) + 1

    if current_chunk:
        yield "\n".join(current_chunk)


def summarize_text(
    url: str, text: str, question: str, driver: Optional[WebDriver] = None
) -> str:
    """Summarize text using the OpenAI API

    Args:
        url (str): The url of the text
        text (str): The text to summarize
        question (str): The question to ask the model
        driver (WebDriver): The webdriver to use to scroll the page

    Returns:
        str: The summary of the text
    """
    if not text:
        return "Error: No text to summarize"

    summaries = []
    chunks = list(split_text(text))
    scroll_ratio = 1 / len(chunks)

    print(f"Summarizing url: {url} with total chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        if driver:
            scroll_to_percentage(driver, scroll_ratio * i)

        #memory_to_add = f"Source: {url}\n" f"Raw content part#{i + 1}: {chunk}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

        messages = [create_message(chunk, question)]

        summary = create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
            max_tokens=CFG.summary_token_limit
        )
        summaries.append(summary)
        #memory_to_add = f"Source: {url}\n" f"Content summary part#{i + 1}: {summary}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

    combined_summary = "\n".join(summaries)
    messages = [create_message(combined_summary, question)]

    final_summary = create_chat_completion(
        model=CFG.fast_llm_model,
        messages=messages,
        max_tokens=CFG.summary_token_limit
    )
    print("Final summary length: ", len(combined_summary))
    print(final_summary)

    return final_summary


def scroll_to_percentage(driver: WebDriver, ratio: float) -> None:
    """Scroll to a percentage of the page

    Args:
        driver (WebDriver): The webdriver to use
        ratio (float): The percentage to scroll to

    Raises:
        ValueError: If the ratio is not between 0 and 1
    """
    if ratio < 0 or ratio > 1:
        raise ValueError("Percentage should be between 0 and 1")
    driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {ratio});")


def create_message(chunk: str, question: str) -> Dict[str, str]:
    """Create a message for the chat completion

    Args:
        chunk (str): The chunk of text to summarize
        question (str): The question to answer

    Returns:
        Dict[str, str]: The message to send to the chat completion
    """
    return {
        "role": "user",
        "content": f'"""{chunk}""" Using the above text, answer in short the following'
        f' question: "{question}" -- if the question cannot be answered using the text,'
        " simply summarize the text. "
        "Include all factual information, numbers, stats etc if available.",
    }

def write_to_file(filename: str, text: str) -> None:
    """Write text to a file

    Args:
        text (str): The text to write
        filename (str): The filename to write to
    """
    with open(filename, "w") as file:
        file.write(text)

async def write_md_to_pdf(task: str, path: str, text: str) -> None:
    file_path = f"{path}/{task}"
    write_to_file(f"{file_path}.md", text)
    md_to_pdf(f"{file_path}.md", f"{file_path}.pdf")
    print(f"{task} written to {file_path}.pdf")

    encoded_file_path = urllib.parse.quote(f"{file_path}.pdf")

    return encoded_file_path

def read_txt_files(directory):
    all_text = ''

    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r') as file:
                all_text += file.read() + '\n'

    return all_text


def md_to_pdf(input_file, output_file):
    md2pdf(output_file,
           md_content=None,
           md_file_path=input_file,
           css_file_path=None,
           base_url=None)


==============================

-web_scrape.py
"""Selenium web scraping module."""
from __future__ import annotations

import logging
import asyncio
from pathlib import Path
from sys import platform

from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.firefox import GeckoDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from fastapi import WebSocket

import processing.text as summary

from config import Config
from processing.html import extract_hyperlinks, format_hyperlinks

from concurrent.futures import ThreadPoolExecutor


executor = ThreadPoolExecutor()

FILE_DIR = Path(__file__).parent.parent
CFG = Config()


async def async_browse(url: str, question: str, websocket: WebSocket) -> str:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user
        websocket (WebSocketManager): The websocket manager

    Returns:
        str: The answer and links to the user
    """
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=8)

    print(f"WEB_SCRAPE Scraping url {url} with question {question}")
    await websocket.send_json(
        {
            "type": "logs",
            "output": f"ðŸ”Ž Browsing the {url} for relevant about: {question}...",
        }
    )

    try:
        driver, text = await loop.run_in_executor(
            executor, scrape_text_with_selenium, url
        )
        await loop.run_in_executor(executor, add_header, driver)
        summary_text = await loop.run_in_executor(
            executor, summary.summarize_text, url, text, question, driver
        )

        await websocket.send_json(
            {
                "type": "logs",
                "output": f"ðŸ“ Information gathered from url {url}: {summary_text}",
            }
        )

        return f"Information gathered from url {url}: {summary_text}"
    except Exception as e:
        print(f"An error occurred while processing the url {url}: {e}")
        return f"Error processing the url {url}: {e}"


def browse_website(url: str, question: str) -> tuple[str, WebDriver]:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user

    Returns:
        Tuple[str, WebDriver]: The answer and links to the user and the webdriver
    """

    if not url:
        return "A URL was not specified, cancelling request to browse website.", None

    driver, text = scrape_text_with_selenium(url)
    add_header(driver)
    summary_text = summary.summarize_text(url, text, question, driver)

    links = scrape_links_with_selenium(driver, url)

    # Limit links to 5
    if len(links) > 5:
        links = links[:5]

    # write_to_file('research-{0}.txt'.format(url), summary_text + "\nSource Links: {0}\n\n".format(links))

    close_browser(driver)
    return f"Answer gathered from website: {summary_text} \n \n Links: {links}", driver


def scrape_text_with_selenium(url: str) -> tuple[WebDriver, str]:
    """Scrape text from a website using selenium

    Args:
        url (str): The url of the website to scrape

    Returns:
        Tuple[WebDriver, str]: The webdriver and the text scraped from the website
    """
    logging.getLogger("selenium").setLevel(logging.CRITICAL)

    options_available = {
        "chrome": ChromeOptions,
        "safari": SafariOptions,
        "firefox": FirefoxOptions,
    }

    options = options_available[CFG.selenium_web_browser]()
    options.add_argument(f"user-agent={CFG.user_agent}")
    options.add_argument("--headless")
    options.add_argument("--enable-javascript")

    if CFG.selenium_web_browser == "firefox":
        service = Service(executable_path=GeckoDriverManager().install())
        driver = webdriver.Firefox(service=service, options=options)
    elif CFG.selenium_web_browser == "safari":
        # Requires a bit more setup on the users end
        # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari
        driver = webdriver.Safari(options=options)
    else:
        if platform == "linux" or platform == "linux2":
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--remote-debugging-port=9222")
        options.add_argument("--no-sandbox")
        options.add_experimental_option("prefs", {"download_restrictions": 3})
        driver = webdriver.Chrome(options=options)

    driver.get(url)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )

    # Get the HTML content directly from the browser's DOM
    page_source = driver.execute_script("return document.body.outerHTML;")
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    # text = soup.get_text()
    text = get_text(soup)

    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = "\n".join(chunk for chunk in chunks if chunk)

    # Print the scraped text
    print(f"Scraped content from {url}:\n{text}\n")

    return driver, text


def get_text(soup):
    """Get the text from the soup

    Args:
        soup (BeautifulSoup): The soup to get the text from

    Returns:
        str: The text from the soup
    """
    text = ""
    tags = ["h1", "h2", "h3", "h4", "h5", "p"]
    for element in soup.find_all(tags):  # Find all the <p> elements
        text += element.text + "\n\n"
    return text


def scrape_links_with_selenium(driver: WebDriver, url: str) -> list[str]:
    """Scrape links from a website using selenium

    Args:
        driver (WebDriver): The webdriver to use to scrape the links

    Returns:
        List[str]: The links scraped from the website
    """
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    hyperlinks = extract_hyperlinks(soup, url)

    # Limit hyperlinks to 5
    if len(hyperlinks) > 5:
        hyperlinks = hyperlinks[:5]

    return format_hyperlinks(hyperlinks)


def close_browser(driver: WebDriver) -> None:
    """Close the browser

    Args:
        driver (WebDriver): The webdriver to close

    Returns:
        None
    """
    driver.quit()


def add_header(driver: WebDriver) -> None:
    """Add a header to the website

    Args:
        driver (WebDriver): The webdriver to use to add the header

    Returns:
        None
    """
    driver.execute_script(open(f"{FILE_DIR}/js/overlay.js", "r").read())


==============================

-web_search.py
from __future__ import annotations
from googleapiclient.discovery import build
import json

def web_search(query: str, num_results: int = 3) -> str: # int - number of results to include with 1 search
    """Useful for general internet search queries using Google Custom Search."""

    print("WEB_SEARCH web_search started")  # Indicates the start of the function

    # Your Google Custom Search API Key and Search engine ID
    API_KEY = "AIzaSyAJxSc8T8_G6Ysxpg_W8cxwXndWu1tXjfs"
    SEARCH_ENGINE_ID = "9486572ea12954a0d"

    # Initialize the Google Custom Search API client
    service = build("customsearch", "v1", developerKey=API_KEY)
    search_results = []

    if not query:
        print("No query provided for web_search")  # Indicates an early return due to missing query
        return json.dumps(search_results)

    # Query Google Custom Search and limit the results
    print(f"WEB_SEARCH Executing web search for query: {query}")  # Indicates that a query is being made
    results = service.cse().list(q=query, cx=SEARCH_ENGINE_ID, num=num_results).execute()
    
    if not results or 'items' not in results:
        print("No results returned from the search query")  # Indicates a case where the search didn't return items
        return json.dumps(search_results)

    # Extract and return the search results
    for item in results['items']:
        search_results.append({
            "title": item.get("title", ""),
            "link": item.get("link", ""),
            "snippet": item.get("snippet", "")
        })

    print(f"WEB_SEARCH Returning {len(search_results)} search results")  # Indicates the function is about to conclude and return results
    return json.dumps(search_results, ensure_ascii=False, indent=4)

# You can add additional functions or utility methods below if needed


==============================

-writer.py
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from agent.prompts import generate_report_prompt, generate_agent_role_prompt
from config import Config

CFG = Config()

class WriterActor:
    def __init__(self):
        print("WRITER Initializing WriterActor...")
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", generate_agent_role_prompt(agent="Default Agent")),
            ("user", generate_report_prompt(question="{query}", research_summary="{results}"))
        ])

    @property
    def runnable(self):
        return {
            "answer": {
                "query": lambda x: x["query"],
                "results": lambda x: "\n\n".join(x["results"])
              } | self.prompt | self.model | StrOutputParser()
        }


==============================

-web_scrape.py
"""Selenium web scraping module."""
from __future__ import annotations

import logging
import asyncio
from pathlib import Path
from sys import platform

from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.firefox import GeckoDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from fastapi import WebSocket

import processing.text as summary

from config import Config
from processing.html import extract_hyperlinks, format_hyperlinks

from concurrent.futures import ThreadPoolExecutor


executor = ThreadPoolExecutor()

FILE_DIR = Path(__file__).parent.parent
CFG = Config()


async def async_browse(url: str, question: str, websocket: WebSocket) -> str:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user
        websocket (WebSocketManager): The websocket manager

    Returns:
        str: The answer and links to the user
    """
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=8)

    print(f"WEB_SCRAPE Scraping url {url} with question {question}")
    await websocket.send_json(
        {
            "type": "logs",
            "output": f"ðŸ”Ž Browsing the {url} for relevant about: {question}...",
        }
    )

    try:
        driver, text = await loop.run_in_executor(
            executor, scrape_text_with_selenium, url
        )
        await loop.run_in_executor(executor, add_header, driver)
        summary_text = await loop.run_in_executor(
            executor, summary.summarize_text, url, text, question, driver
        )

        await websocket.send_json(
            {
                "type": "logs",
                "output": f"ðŸ“ Information gathered from url {url}: {summary_text}",
            }
        )

        return f"Information gathered from url {url}: {summary_text}"
    except Exception as e:
        print(f"An error occurred while processing the url {url}: {e}")
        return f"Error processing the url {url}: {e}"


def browse_website(url: str, question: str) -> tuple[str, WebDriver]:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user

    Returns:
        Tuple[str, WebDriver]: The answer and links to the user and the webdriver
    """

    if not url:
        return "A URL was not specified, cancelling request to browse website.", None

    driver, text = scrape_text_with_selenium(url)
    add_header(driver)
    summary_text = summary.summarize_text(url, text, question, driver)

    links = scrape_links_with_selenium(driver, url)

    # Limit links to 5
    if len(links) > 5:
        links = links[:5]

    # write_to_file('research-{0}.txt'.format(url), summary_text + "\nSource Links: {0}\n\n".format(links))

    close_browser(driver)
    return f"Answer gathered from website: {summary_text} \n \n Links: {links}", driver


def scrape_text_with_selenium(url: str) -> tuple[WebDriver, str]:
    """Scrape text from a website using selenium

    Args:
        url (str): The url of the website to scrape

    Returns:
        Tuple[WebDriver, str]: The webdriver and the text scraped from the website
    """
    logging.getLogger("selenium").setLevel(logging.CRITICAL)

    options_available = {
        "chrome": ChromeOptions,
        "safari": SafariOptions,
        "firefox": FirefoxOptions,
    }

    options = options_available[CFG.selenium_web_browser]()
    options.add_argument(f"user-agent={CFG.user_agent}")
    options.add_argument("--headless")
    options.add_argument("--enable-javascript")

    if CFG.selenium_web_browser == "firefox":
        service = Service(executable_path=GeckoDriverManager().install())
        driver = webdriver.Firefox(service=service, options=options)
    elif CFG.selenium_web_browser == "safari":
        # Requires a bit more setup on the users end
        # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari
        driver = webdriver.Safari(options=options)
    else:
        if platform == "linux" or platform == "linux2":
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--remote-debugging-port=9222")
        options.add_argument("--no-sandbox")
        options.add_experimental_option("prefs", {"download_restrictions": 3})
        driver = webdriver.Chrome(options=options)

    driver.get(url)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )

    # Get the HTML content directly from the browser's DOM
    page_source = driver.execute_script("return document.body.outerHTML;")
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    # text = soup.get_text()
    text = get_text(soup)

    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = "\n".join(chunk for chunk in chunks if chunk)

    # Print the scraped text
    print(f"Scraped content from {url}:\n{text}\n")

    return driver, text


def get_text(soup):
    """Get the text from the soup

    Args:
        soup (BeautifulSoup): The soup to get the text from

    Returns:
        str: The text from the soup
    """
    text = ""
    tags = ["h1", "h2", "h3", "h4", "h5", "p"]
    for element in soup.find_all(tags):  # Find all the <p> elements
        text += element.text + "\n\n"
    return text


def scrape_links_with_selenium(driver: WebDriver, url: str) -> list[str]:
    """Scrape links from a website using selenium

    Args:
        driver (WebDriver): The webdriver to use to scrape the links

    Returns:
        List[str]: The links scraped from the website
    """
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    hyperlinks = extract_hyperlinks(soup, url)

    # Limit hyperlinks to 5
    if len(hyperlinks) > 5:
        hyperlinks = hyperlinks[:5]

    return format_hyperlinks(hyperlinks)


def close_browser(driver: WebDriver) -> None:
    """Close the browser

    Args:
        driver (WebDriver): The webdriver to close

    Returns:
        None
    """
    driver.quit()


def add_header(driver: WebDriver) -> None:
    """Add a header to the website

    Args:
        driver (WebDriver): The webdriver to use to add the header

    Returns:
        None
    """
    driver.execute_script(open(f"{FILE_DIR}/js/overlay.js", "r").read())


==============================

-web_search.py
from __future__ import annotations
from googleapiclient.discovery import build
import json

def web_search(query: str, num_results: int = 3) -> str: # int - number of results to include with 1 search
    """Useful for general internet search queries using Google Custom Search."""

    print("WEB_SEARCH web_search started")  # Indicates the start of the function

    # Your Google Custom Search API Key and Search engine ID
    API_KEY = "AIzaSyAJxSc8T8_G6Ysxpg_W8cxwXndWu1tXjfs"
    SEARCH_ENGINE_ID = "9486572ea12954a0d"

    # Initialize the Google Custom Search API client
    service = build("customsearch", "v1", developerKey=API_KEY)
    search_results = []

    if not query:
        print("No query provided for web_search")  # Indicates an early return due to missing query
        return json.dumps(search_results)

    # Query Google Custom Search and limit the results
    print(f"WEB_SEARCH Executing web search for query: {query}")  # Indicates that a query is being made
    results = service.cse().list(q=query, cx=SEARCH_ENGINE_ID, num=num_results).execute()
    
    if not results or 'items' not in results:
        print("No results returned from the search query")  # Indicates a case where the search didn't return items
        return json.dumps(search_results)

    # Extract and return the search results
    for item in results['items']:
        search_results.append({
            "title": item.get("title", ""),
            "link": item.get("link", ""),
            "snippet": item.get("snippet", "")
        })

    print(f"WEB_SEARCH Returning {len(search_results)} search results")  # Indicates the function is about to conclude and return results
    return json.dumps(search_results, ensure_ascii=False, indent=4)

# You can add additional functions or utility methods below if needed


==============================

-llm_utils.py
from __future__ import annotations

import json

from fastapi import WebSocket
import time

import openai
from langchain.adapters import openai as lc_openai
from colorama import Fore, Style
from openai.error import APIError, RateLimitError

from agent.prompts import auto_agent_instructions
from config import Config

CFG = Config()

openai.api_key = CFG.openai_api_key

from typing import Optional
import logging


def create_chat_completion(
    messages: list,  # type: ignore
    model: Optional[str] = None,
    temperature: float = CFG.temperature,
    max_tokens: Optional[int] = None,
    stream: Optional[bool] = False,
    websocket: WebSocket | None = None,
) -> str:
    """Create a chat completion using the OpenAI API
    Args:
        messages (list[dict[str, str]]): The messages to send to the chat completion
        model (str, optional): The model to use. Defaults to None.
        temperature (float, optional): The temperature to use. Defaults to 0.9.
        max_tokens (int, optional): The max tokens to use. Defaults to None.
        stream (bool, optional): Whether to stream the response. Defaults to False.
    Returns:
        str: The response from the chat completion
    """

    # validate input
    if model is None:
        raise ValueError("Model cannot be None")
    if max_tokens is not None and max_tokens > 8001:
        raise ValueError(f"Max tokens cannot be more than 8001, but got {max_tokens}")
    if stream and websocket is None:
        raise ValueError("Websocket cannot be None when stream is True")

    # create response
    for attempt in range(10):  # maximum of 10 attempts
        response = send_chat_completion_request(
            messages, model, temperature, max_tokens, stream, websocket
        )
        return response

    logging.error("Failed to get response from OpenAI API")
    raise RuntimeError("Failed to get response from OpenAI API")


def send_chat_completion_request(
    messages, model, temperature, max_tokens, stream, websocket
):
    if not stream:
        result = lc_openai.ChatCompletion.create(
            model=model,  # Change model here to use different models
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            provider=CFG.llm_provider,  # Change provider here to use a different API
        )
        return result["choices"][0]["message"]["content"]
    else:
        return stream_response(model, messages, temperature, max_tokens, websocket)


async def stream_response(model, messages, temperature, max_tokens, websocket):
    paragraph = ""
    response = ""
    print(f"LLM_UTILS streaming response...")

    for chunk in lc_openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        provider=CFG.llm_provider,
        stream=True,
    ):
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content is not None:
            response += content
            paragraph += content
            if "\n" in paragraph:
                await websocket.send_json({"type": "report", "output": paragraph})
                paragraph = ""
    print(f"LLM_UTILS streaming response complete")
    return response


def choose_agent(task: str) -> dict:
    """Determines what agent should be used
    Args:
        task (str): The research question the user asked
    Returns:
        agent - The agent that will be used
        agent_role_prompt (str): The prompt for the agent
    """
    try:
        response = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{auto_agent_instructions()}"},
                {"role": "user", "content": f"task: {task}"},
            ],
            temperature=0,
        )

        return json.loads(response)
    except Exception as e:
        print(f"{Fore.RED}Error in choose_agent: {e}{Style.RESET_ALL}")
        return {
            "agent": "Default Agent",
            "agent_role_prompt": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
        }


==============================

-prompts.py
from datetime import datetime


def generate_agent_role_prompt(agent):
    """Generates the agent role prompt.
    Args: agent (str): The type of the agent.
    Returns: str: The agent role prompt.
    """
    prompts = {
        "Finance Agent": "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.",
        "Travel Agent": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.",
        "Academic Research Agent": "You are an AI academic research assistant. Your primary responsibility is to create thorough, academically rigorous, unbiased, and systematically organized reports on a given research topic, following the standards of scholarly work.",
        "Business Analyst": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.",
        "Computer Security Analyst Agent": "You are an AI specializing in computer security analysis. Your principal duty is to generate comprehensive, meticulously detailed, impartial, and systematically structured reports on computer security topics. This includes Exploits, Techniques, Threat Actors, and Advanced Persistent Threat (APT) Groups. All produced reports should adhere to the highest standards of scholarly work and provide in-depth insights into the complexities of computer security.",
        "Default Agent": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
    }

    return prompts.get(agent, "No such agent")


def generate_report_prompt(question, research_summary):
    """Generates the report prompt for the given question and research summary.
    Args: question (str): The question to generate the report prompt for
            research_summary (str): The research summary to generate the report prompt for
    Returns: str: The report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, answer the following'
        f' question or topic: "{question}" in a detailed report --'
        " The report should focus on the answer to the question, should be well structured, informative,"
        " in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format.\n "
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n"
        f"Write all used source urls at the end of the report in apa format.\n "
        f"Assume that the current date is {datetime.now().strftime('%B %d, %Y')}"
    )


def generate_search_queries_prompt(question):
    """Generates the search queries prompt for the given question.
    Args: question (str): The question to generate the search queries prompt for
    Returns: str: The search queries prompt for the given question
    """
    print(
        f"PROMPTS Generating search queries prompt for: {question}"
    )  # Indicates the specific question being processed for search queries

    queries = (
        f'Write 3 google search queries to search online that form an objective opinion from the following: "{question}"'
        f'Use the current date if needed: {datetime.now().strftime("%B %d, %Y")}.\n'
        f'You must respond with a list of strings in the following format: ["query 1", "query 2", "query 3"].'
    )
    print(f"PROMPTS Generated search queries: {queries}")  # Print the queries

    return queries


def generate_resource_report_prompt(question, research_summary):
    """Generates the resource report prompt for the given question and research summary.

    Args:
        question (str): The question to generate the resource report prompt for.
        research_summary (str): The research summary to generate the resource report prompt for.

    Returns:
        str: The resource report prompt for the given question and research summary.
    """
    return (
        f'"""{research_summary}""" Based on the above information, generate a bibliography recommendation report for the following'
        f' question or topic: "{question}". The report should provide a detailed analysis of each recommended resource,'
        " explaining how each source can contribute to finding answers to the research question."
        " Focus on the relevance, reliability, and significance of each source."
        " Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax."
        " Include relevant facts, figures, and numbers whenever available."
        " The report should have a minimum length of 1,200 words."
    )


def generate_outline_report_prompt(question, research_summary):
    """Generates the outline report prompt for the given question and research summary.
    Args: question (str): The question to generate the outline report prompt for
            research_summary (str): The research summary to generate the outline report prompt for
    Returns: str: The outline report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, generate an outline for a research report in Markdown syntax'
        f' for the following question or topic: "{question}". The outline should provide a well-structured framework'
        " for the research report, including the main sections, subsections, and key points to be covered."
        " The research report should be detailed, informative, in-depth, and a minimum of 1,200 words."
        " Use appropriate Markdown syntax to format the outline and ensure readability."
    )


def generate_concepts_prompt(question, research_summary):
    """Generates the concepts prompt for the given question.
    Args: question (str): The question to generate the concepts prompt for
            research_summary (str): The research summary to generate the concepts prompt for
    Returns: str: The concepts prompt for the given question
    """

    return (
        f'"""{research_summary}""" Using the above information, generate a list of 5 main concepts to learn for a research report'
        f' on the following question or topic: "{question}". The outline should provide a well-structured framework'
        'You must respond with a list of strings in the following format: ["concepts 1", "concepts 2", "concepts 3", "concepts 4, concepts 5"]'
    )


def generate_lesson_prompt(concept):
    """
    Generates the lesson prompt for the given question.
    Args:
        concept (str): The concept to generate the lesson prompt for.
    Returns:
        str: The lesson prompt for the given concept.
    """

    prompt = (
        f"generate a comprehensive lesson about {concept} in Markdown syntax. This should include the definition"
        f"of {concept}, its historical background and development, its applications or uses in different"
        f"fields, and notable events or facts related to {concept}."
    )

    return prompt


def get_report_by_type(report_type):
    report_type_mapping = {
        "research_report": generate_report_prompt,
        "resource_report": generate_resource_report_prompt,
        "outline_report": generate_outline_report_prompt,
    }
    return report_type_mapping[report_type]


def auto_agent_instructions():
    return """
        This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific agent, defined by its type and role, with each agent requiring distinct instructions.
        Agent
        The agent is determined by the field of the topic and the specific name of the agent that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each agent type is associated with a corresponding emoji.

        examples:
        task: "should I invest in apple stocks?"
        response: 
        {
            "agent": "ðŸ’° Finance Agent",
            "agent_role_prompt: "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends."
        }
        task: "could reselling sneakers become profitable?"
        response: 
        { 
            "agent":  "ðŸ“ˆ Business Analyst Agent",
            "agent_role_prompt": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis."
        }
        task: "what are the most interesting sites in Tel Aviv?"
        response:
        {
            "agent:  "ðŸŒ Travel Agent",
            "agent_role_prompt": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights."
        }
    """


==============================

-research_agent.py
# Description: Research assistant class that handles the research process for a given question.

# libraries
import asyncio
import json
import hashlib
from actions.web_search import web_search
from actions.web_scrape import async_browse
from processing.text import (
    write_to_file,
    create_message,
    create_chat_completion,
    read_txt_files,
    write_md_to_pdf,
)
from config import Config
from agent import prompts
import os
import string

CFG = Config()


class ResearchAgent:
    def __init__(self, question, agent, agent_role_prompt, websocket=None):
        """Initializes the research assistant with the given question.
        Args: question (str): The question to research
        Returns: None
        """
        self.question = question
        self.agent = agent
        self.agent_role_prompt = (
            agent_role_prompt
            if agent_role_prompt
            else prompts.generate_agent_role_prompt(agent)
        )
        self.visited_urls = set()
        self.research_summary = ""
        self.dir_path = f"./outputs/{hashlib.sha1(question.encode()).hexdigest()}"
        self.websocket = websocket

        print("RESEARCH_AGENT ResearchAgent initialized with question:", self.question)

    async def stream_output(self, output):
        if not self.websocket:
            return print(output)
        await self.websocket.send_json({"type": "logs", "output": output})

    async def summarize(self, text, topic):
        """Summarizes the given text for the given topic.
        Args: text (str): The text to summarize
                topic (str): The topic to summarize the text for
        Returns: str: The summarized text
        """
        print("RESEARCH_AGENT Starting summarization for topic:", topic)

        messages = [create_message(text, topic)]
        await self.stream_output(f"ðŸ“ Summarizing text for query: {text}")

        return create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
        )

    async def get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """
        new_urls = []
        for url in url_set_input:
            if url not in self.visited_urls:
                await self.stream_output(f"âœ… Adding source url to research: {url}\n")

                self.visited_urls.add(url)
                new_urls.append(url)

        return new_urls

    async def call_agent(self, action, stream=False, websocket=None):
        messages = [
            {"role": "system", "content": self.agent_role_prompt},
            {
                "role": "user",
                "content": action,
            },
        ]
        answer = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=messages,
            stream=stream,
            websocket=websocket,
        )
        return answer

    async def create_search_queries(self):
        """Modifies the method to use the user's input directly as the search query.
        Args: None
        Returns: list[str]: The search queries for the given question
        """
        # Directly use the user's input (self.question) as the search query
        await self.stream_output(
            f"ðŸ§  I will conduct my research based on the following query: '{self.question}'..."
        )
        return [self.question]

    async def async_search(self, query):
        """Runs the async search for the given query.
        Args: query (str): The query to run the async search for
        Returns: list[str]: The async search for the given query
        """
        search_results = json.loads(web_search(query))
        new_search_urls = await self.get_new_urls([url.get("link") for url in search_results])

        await self.stream_output(
            f"ðŸŒ Browsing the following sites for relevant information: {new_search_urls}..."
        )

        # Create a list to hold the coroutine objects
        tasks = [
            async_browse(url, query, self.websocket) for url in new_search_urls
        ]

        # Gather the results as they become available
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        return responses

    async def run_search_summary(self, query):
        """Runs the search summary for the given query.
        Args: query (str): The query to run the search summary for
        Returns: str: The search summary for the given query
        """
        await self.stream_output(f"ðŸ”Ž Running research for '{query}'...")

        responses = await self.async_search(query)

        result = "\n".join(responses)
        os.makedirs(
            os.path.dirname(f"{self.dir_path}/research-{query}.txt"), exist_ok=True
        )
        write_to_file(f"{self.dir_path}/research-{query}.txt", result)
        return result

    async def conduct_research(self):
        """Conducts the research for the given question.
        Args: None
        Returns: str: The research for the given question
        """
        self.research_summary = (
            read_txt_files(self.dir_path) if os.path.isdir(self.dir_path) else ""
        )

        if not self.research_summary:
            search_queries = await self.create_search_queries()
            for query in search_queries:
                research_result = await self.run_search_summary(query)
                self.research_summary += f"{research_result}\n\n"

        await self.stream_output(
            f"Total research words: {len(self.research_summary.split(' '))}"
        )

        return self.research_summary

    async def create_concepts(self):
        """Creates the concepts for the given question.
        Args: None
        Returns: list[str]: The concepts for the given question
        """
        result = await self.call_agent(
            prompts.generate_concepts_prompt(self.question, self.research_summary)
        )

        await self.stream_output(
            f"I will research based on the following concepts: {result}\n"
        )
        return json.loads(result)

    async def write_report(self, report_type, websocket=None):
        """Writes the report for the given question.
        Args: None
        Returns: str: The report for the given question
        """
        print("RESEARCH_AGENT Starting to write report of type:", report_type)

        report_type_func = prompts.get_report_by_type(report_type)
        await self.stream_output(
            f"âœï¸ Writing {report_type} for research task: {self.question}..."
        )

        answer = await self.call_agent(
            report_type_func(self.question, self.research_summary),
            stream=websocket is not None,
            websocket=websocket,
        )
        # if websocket is True than we are streaming gpt response, so we need to wait for the final response
        final_report = await answer if websocket else answer

        path = await write_md_to_pdf(report_type, self.dir_path, final_report)

        return answer, path

    async def write_lessons(self):
        """Writes lessons on essential concepts of the research.
        Args: None
        Returns: None
        """
        concepts = await self.create_concepts()
        for concept in concepts:
            answer = await self.call_agent(
                prompts.generate_lesson_prompt(concept), stream=True
            )
            await write_md_to_pdf("Lesson", self.dir_path, answer)


==============================

-run.py
#manage WebSocket connections and the execution of an agent's research task
import asyncio
import datetime

from typing import List, Dict
from fastapi import WebSocket
from config import check_openai_api_key
from agent.research_agent import ResearchAgent


class WebSocketManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}
        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}

    async def start_sender(self, websocket: WebSocket):
        queue = self.message_queues[websocket]
        while True:
            message = await queue.get()
            if websocket in self.active_connections:
                await websocket.send_text(message)
            else:
                break

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.message_queues[websocket] = asyncio.Queue()
        self.sender_tasks[websocket] = asyncio.create_task(self.start_sender(websocket))

    async def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        self.sender_tasks[websocket].cancel()
        del self.sender_tasks[websocket]
        del self.message_queues[websocket]

    async def start_streaming(self, task, report_type, agent, agent_role_prompt, websocket):
        report, path = await run_agent(task, report_type, agent, agent_role_prompt, websocket)
        return report, path


async def run_agent(task, report_type, agent, agent_role_prompt, websocket):
    check_openai_api_key()

    start_time = datetime.datetime.now()

    # await websocket.send_json({"type": "logs", "output": f"Start time: {str(start_time)}\n\n"})

    assistant = ResearchAgent(task, agent, agent_role_prompt, websocket)
    await assistant.conduct_research()

    report, path = await assistant.write_report(report_type, websocket)

    await websocket.send_json({"type": "path", "output": path})

    end_time = datetime.datetime.now()
    await websocket.send_json({"type": "logs", "output": f"\nEnd time: {end_time}\n"})
    await websocket.send_json({"type": "logs", "output": f"\nTotal run time: {end_time - start_time}\n"})

    return report, path


==============================

-scripts.js
const GPTResearcher = (() => {
    const init = () => {
      // Not sure, but I think it would be better to add event handlers here instead of in the HTML
      //document.getElementById("startResearch").addEventListener("click", startResearch);
      //document.getElementById("copyToClipboard").addEventListener("click", copyToClipboard);

      updateState("initial");
    }

    const startResearch = () => {
      document.getElementById("output").innerHTML = "";
      document.getElementById("reportContainer").innerHTML = "";
      updateState("in_progress")
  
      addAgentResponse({ output: "ðŸ¤” Thinking about research questions for the task..." });
  
      listenToSockEvents();
    };
  
    const listenToSockEvents = () => {
      const { protocol, host, pathname } = window.location;
      const ws_uri = `${protocol === 'https:' ? 'wss:' : 'ws:'}//${host}${pathname}ws`;
      const converter = new showdown.Converter();
      const socket = new WebSocket(ws_uri);
  
      socket.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.type === 'logs') {
          addAgentResponse(data);
        } else if (data.type === 'report') {
          writeReport(data, converter);
        } else if (data.type === 'path') {
          updateState("finished")
          updateDownloadLink(data);

        }
      };
  
      socket.onopen = (event) => {
        const task = document.querySelector('input[name="task"]').value;
        const report_type = document.querySelector('select[name="report_type"]').value;
        const agent = document.querySelector('input[name="agent"]:checked').value;
  
        const requestData = {
          task: task,
          report_type: report_type,
          agent: agent,
        };
  
        socket.send(`start ${JSON.stringify(requestData)}`);
      };
    };
  
    const addAgentResponse = (data) => {
      const output = document.getElementById("output");
      output.innerHTML += '<div class="agent_response">' + data.output + '</div>';
      output.scrollTop = output.scrollHeight;
      output.style.display = "block";
      updateScroll();
    };
  
    const writeReport = (data, converter) => {
      const reportContainer = document.getElementById("reportContainer");
      const markdownOutput = converter.makeHtml(data.output);
      reportContainer.innerHTML += markdownOutput;
      updateScroll();
    };
  
    const updateDownloadLink = (data) => {
      const path = data.output;
      document.getElementById("downloadLink").setAttribute("href", path);
    };
  
    const updateScroll = () => {
      window.scrollTo(0, document.body.scrollHeight);
    };
  
    const copyToClipboard = () => {
      const textarea = document.createElement('textarea');
      textarea.id = 'temp_element';
      textarea.style.height = 0;
      document.body.appendChild(textarea);
      textarea.value = document.getElementById('reportContainer').innerText;
      const selector = document.querySelector('#temp_element');
      selector.select();
      document.execCommand('copy');
      document.body.removeChild(textarea);
    };

    const updateState = (state) => {
      var status = "";
      switch (state) {
        case "in_progress":
          status = "Research in progress..."
          setReportActionsStatus("disabled");
          break;
        case "finished":
          status = "Research finished!"
          setReportActionsStatus("enabled");
          break;
        case "error":
          status = "Research failed!"
          setReportActionsStatus("disabled");
          break;
        case "initial":
          status = ""
          setReportActionsStatus("hidden");
          break;
        default:
          setReportActionsStatus("disabled");
      }
      document.getElementById("status").innerHTML = status;
      if (document.getElementById("status").innerHTML == "") {
        document.getElementById("status").style.display = "none";
      } else {
        document.getElementById("status").style.display = "block";
      }
    }

    /**
     * Shows or hides the download and copy buttons
     * @param {str} status Kind of hacky. Takes "enabled", "disabled", or "hidden". "Hidden is same as disabled but also hides the div"
     */
    const setReportActionsStatus = (status) => {
      const reportActions = document.getElementById("reportActions");
      // Disable everything in reportActions until research is finished

      if (status == "enabled") {
        reportActions.querySelectorAll("a").forEach((link) => {
          link.classList.remove("disabled");
          link.removeAttribute('onclick');
          reportActions.style.display = "block";
        });
      } else {
        reportActions.querySelectorAll("a").forEach((link) => {
          link.classList.add("disabled");
          link.setAttribute('onclick', "return false;");
        });
        if (status == "hidden") {
          reportActions.style.display = "none";
        }
      }
    }

    document.addEventListener("DOMContentLoaded", init);
    return {
      startResearch,
      copyToClipboard,
    };
  })();

==============================

-__init__.py
from config.config import Config, check_openai_api_key
from config.singleton import AbstractSingleton, Singleton

__all__ = [
    "check_openai_api_key",
    "AbstractSingleton",
    "Config",
    "Singleton",
]


==============================

-config.py
"""Configuration class to store the state of bools for different scripts access."""
import os

import openai
from colorama import Fore
from dotenv import load_dotenv

from config.singleton import Singleton

load_dotenv(verbose=True)


class Config(metaclass=Singleton):
    """
    Configuration class to store the state of bools for different scripts access.
    """

    def __init__(self) -> None:
        """Initialize the Config class"""
        self.debug_mode = False
        self.allow_downloads = False

        self.selenium_web_browser = os.getenv("USE_WEB_BROWSER", "chrome")
        self.llm_provider = os.getenv("LLM_PROVIDER", "ChatOpenAI")
        self.fast_llm_model = os.getenv("FAST_LLM_MODEL", "gpt-3.5-turbo-16k")
        self.smart_llm_model = os.getenv("SMART_LLM_MODEL", "gpt-4")
        self.fast_token_limit = int(os.getenv("FAST_TOKEN_LIMIT", 2000))
        self.smart_token_limit = int(os.getenv("SMART_TOKEN_LIMIT", 4000))
        self.browse_chunk_max_length = int(os.getenv("BROWSE_CHUNK_MAX_LENGTH", 8192))
        self.summary_token_limit = int(os.getenv("SUMMARY_TOKEN_LIMIT", 700))

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.temperature = float(os.getenv("TEMPERATURE", "1"))

        self.user_agent = os.getenv(
            "USER_AGENT",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36"
            " (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36",
        )

        self.memory_backend = os.getenv("MEMORY_BACKEND", "local")
        # Initialize the OpenAI API client
        openai.api_key = self.openai_api_key

    def set_fast_llm_model(self, value: str) -> None:
        """Set the fast LLM model value."""
        self.fast_llm_model = value

    def set_smart_llm_model(self, value: str) -> None:
        """Set the smart LLM model value."""
        self.smart_llm_model = value

    def set_fast_token_limit(self, value: int) -> None:
        """Set the fast token limit value."""
        self.fast_token_limit = value

    def set_smart_token_limit(self, value: int) -> None:
        """Set the smart token limit value."""
        self.smart_token_limit = value

    def set_browse_chunk_max_length(self, value: int) -> None:
        """Set the browse_website command chunk max length value."""
        self.browse_chunk_max_length = value

    def set_openai_api_key(self, value: str) -> None:
        """Set the OpenAI API key value."""
        self.openai_api_key = value

    def set_debug_mode(self, value: bool) -> None:
        """Set the debug mode value."""
        self.debug_mode = value


def check_openai_api_key() -> None:
    """Check if the OpenAI API key is set in config.py or as an environment variable."""
    cfg = Config()
    if not cfg.openai_api_key:
        print(
            Fore.RED
            + "Please set your OpenAI API key in .env or as an environment variable."
        )
        print("You can get your key from https://platform.openai.com/account/api-keys")
        exit(1)


==============================

-singleton.py
"""The singleton metaclass for ensuring only one instance of a class."""
import abc


class Singleton(abc.ABCMeta, type):
    """
    Singleton metaclass for ensuring only one instance of a class.
    """

    _instances = {}

    def __call__(cls, *args, **kwargs):
        """Call method for the singleton metaclass."""
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class AbstractSingleton(abc.ABC, metaclass=Singleton):
    """
    Abstract singleton class for ensuring only one instance of a class.
    """

    pass


==============================

-overlay.js
const overlay = document.createElement('div');
Object.assign(overlay.style, {
    position: 'fixed',
    zIndex: 999999,
    top: 0,
    left: 0,
    width: '100%',
    height: '100%',
    background: 'rgba(0, 0, 0, 0.7)',
    color: '#fff',
    fontSize: '24px',
    fontWeight: 'bold',
    display: 'flex',
    justifyContent: 'center',
    alignItems: 'center',
});
const textContent = document.createElement('div');
Object.assign(textContent.style, {
    textAlign: 'center',
});
textContent.textContent = 'Tavily AI: Analyzing Page';
overlay.appendChild(textContent);
document.body.append(overlay);
document.body.style.overflow = 'hidden';
let dotCount = 0;
setInterval(() => {
    textContent.textContent = 'Tavily AI: Analyzing Page' + '.'.repeat(dotCount);
    dotCount = (dotCount + 1) % 4;
}, 1000);


==============================

-main.py
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import os

from agent.llm_utils import choose_agent
from agent.run import WebSocketManager


class ResearchRequest(BaseModel):
    task: str
    report_type: str
    agent: str



app = FastAPI()
app.mount("/site", StaticFiles(directory="client"), name="site")
app.mount("/static", StaticFiles(directory="client/static"), name="static")
# Dynamic directory for outputs once first research is run
@app.on_event("startup")
def startup_event():
    if not os.path.isdir("outputs"):
        os.makedirs("outputs")
    app.mount("/outputs", StaticFiles(directory="outputs"), name="outputs")

templates = Jinja2Templates(directory="client")

manager = WebSocketManager()


@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse('index.html', {"request": request, "report": None})


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            if data.startswith("start"):
                json_data = json.loads(data[6:])
                task = json_data.get("task")
                report_type = json_data.get("report_type")
                agent = json_data.get("agent")
                # temporary so "normal agents" can still be used and not just auto generated, will be removed when we move to auto generated
                if agent == "Auto Agent":
                    agent_dict = choose_agent(task)
                    agent = agent_dict.get("agent")
                    agent_role_prompt = agent_dict.get("agent_role_prompt")
                else:
                    agent_role_prompt = None

                await websocket.send_json({"type": "logs", "output": f"Initiated an Agent: {agent}"})
                if task and report_type and agent:
                    await manager.start_streaming(task, report_type, agent, agent_role_prompt, websocket)
                else:
                    print("Error: not enough parameters provided.")

    except WebSocketDisconnect:
        await manager.disconnect(websocket)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)


==============================

-editor.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

EDIT_TEMPLATE = """You are an editor. \
You have been tasked with editing the following draft, which was written by a non-expert. \
Please accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision. \
Things you should be checking for:

- This draft MUST fully answer the original question
- This draft MUST be written in apa format

If not all of the above criteria are met, you should send appropriate revision notes.
"""


class EditorActor:
    def __init__(self):
        print("EDITOR Initializing EditorActor")  # Indicates the start of the EditorActor initialization

        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(EDIT_TEMPLATE) + "Draft:\n\n{draft}"
        self.functions = [
            {
                "name": "revise",
                "description": "Sends the draft for revision",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "notes": {
                            "type": "string",
                            "description": "The editor's notes to guide the revision.",
                        },
                    },
                },
            },
            {
                "name": "accept",
                "description": "Accepts the draft",
                "parameters": {
                    "type": "object",
                    "properties": {"ready": {"const": True}},
                },
            },
        ]

    @property
    def runnable(self):
        return (
            self.prompt | self.model.bind(functions=self.functions)
        )


==============================

-research_team.py
from operator import itemgetter
from langchain.runnables.openai_functions import OpenAIFunctionsRouter

from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic

"""
    This is the research team. 
    It is a group of autonomous agents that work together to answer a given question
    using a comprehensive research process that includes: 
        - Searching for relevant information across multiple sources
        - Extracting relevant information
        - Writing a well structured report
        - Validating the report
        - Revising the report
        - Repeat until the report is satisfactory
"""


class ResearchTeam:
    def __init__(self, research_actor, editor_actor, reviser_actor):
        self.research_actor_instance = research_actor
        self.editor_actor_instance = editor_actor
        self.revise_actor_instance = reviser_actor

    def run(self, query):
        print("ResearchTeam: Initialized with query:", query)  # Starting the research process

        # create topics
        editor_inbox = Topic("editor_inbox")
        reviser_inbox = Topic("reviser_inbox")
        print("ResearchTeam: Topics created - editor_inbox, reviser_inbox")

        research_chain = (
            # Listed in inputs
            Topic.IN.subscribe()
            | {"draft": lambda x: self.research_actor_instance.run(x["question"])}
            # The draft always goes to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Research chain initialized")

        editor_chain = (
            # Listen for events in the editor_inbox
            editor_inbox.subscribe()
            | self.editor_actor_instance.runnable
            # Depending on the output, different things should happen
            | OpenAIFunctionsRouter(
                {
                    # If revise is chosen, we send a push to the critique_inbox
                    "revise": (
                        {
                            "notes": itemgetter("notes"),
                            "draft": editor_inbox.current() | itemgetter("draft"),
                            "question": Topic.IN.current() | itemgetter("question"),
                        }
                        | reviser_inbox.publish()
                    ),
                    # If accepted, then we return
                    "accept": editor_inbox.current() | Topic.OUT.publish(),
                }
            )
        )
        print("ResearchTeam: Editor chain initialized")

        reviser_chain = (
            # Listen for events in the reviser's inbox
            reviser_inbox.subscribe()
            | self.revise_actor_instance.runnable
            # Publish to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Reviser chain initialized")

        web_researcher = PubSub(
            research_chain,
            editor_chain,
            reviser_chain,
            connection=InMemoryPubSubConnection(),
        )
        print("ResearchTeam: PubSub initialized with all chains")

        res = web_researcher.invoke({"question": query})
        print("ResearchTeam: Invocation complete with result:", res)

        return res["draft"]


==============================

-researcher.py
from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic


class Researcher:
    def __init__(self, search_actor, writer_actor):
        print("RESEARCHER Initializing Researcher...")
        self.search_actor_instance = search_actor
        self.writer_actor_instance = writer_actor

    def run(self, query):
        # The research inbox
        research_inbox = Topic("research")
        search_actor = (
            Topic.IN.subscribe()
            | {"query": lambda x: x, "results": self.search_actor_instance.runnable}
            | research_inbox.publish()
        )

        write_actor = (
            research_inbox.subscribe()
            | self.writer_actor_instance.runnable
            | Topic.OUT.publish()
        )

        researcher = PubSub(
            search_actor,
            write_actor,
            connection=InMemoryPubSubConnection(),
        )

        res = researcher.invoke(query)
        return res["answer"]


==============================

-reviser.py
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

class ReviserActor:
    def __init__(self):
        print("REVISER Initializing ReviserActor")  # Indicates the start of the EditorActor initialization

        
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(
            "You are an expert writer. "
            "You have been tasked by your editor with revising the following draft, which was written by a non-expert. "
            "You may follow the editor's notes or not, as you see fit."
        ) + "Draft:\n\n{draft}" + "Editor's notes:\n\n{notes}"

    @property
    def runnable(self):
        return {
            "draft": {
                "draft": lambda x: x["draft"],
                "notes": lambda x: x["notes"],
            } | self.prompt | self.model | StrOutputParser()
        }


==============================

-gpt_researcher.py
import json
# Import necessary modules for text processing, web scraping, and searching
from processing.text import summarize_text
from actions.web_scrape import scrape_text_with_selenium
from actions.web_search import web_search

# Import Langchain and related utilities for AI-driven chat and prompt management
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableMap, RunnableLambda
from langchain.schema.messages import SystemMessage
from agent.prompts import auto_agent_instructions, generate_search_queries_prompt
from config import Config

# Load global configurations
CFG = Config()

# [Initiation] Prepare the AI-driven message template for generating search queries
search_message = (generate_search_queries_prompt("{question}"))
SEARCH_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "{agent_prompt}"),
    ("user", search_message)
])

# Load instructions for automated agent behavior
AUTO_AGENT_INSTRUCTIONS = auto_agent_instructions()
CHOOSE_AGENT_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessage(content=AUTO_AGENT_INSTRUCTIONS),
    ("user", "task: {task}")
])

# [Content Retrieval and Summarization and Analysis] Define the process for scraping and summarizing text from a URL
scrape_and_summarize = {
    "question": lambda x: x["question"],
    "text": lambda x: scrape_text_with_selenium(x['url'])[1],
    "url": lambda x: x['url']
} | RunnableMap({
        "summary": lambda x: summarize_text(text=x["text"], question=x["question"], url=x["url"]),
        "url": lambda x: x['url']
}) | (lambda x: f"Source Url: {x['url']}\nSummary: {x['summary']}")

# Initialize a set to keep track of URLs that have already been seen to avoid duplicate content
seen_urls = set()

# [Web Search and Content Retrieval] Define the process for conducting multiple searches, avoiding duplicate URLs, and processing the results
multi_search = (
    lambda x: [
        {"url": url.get("href"), "question": x["question"]}
        for url in json.loads(web_search(query=x["question"], num_results=3))
        if not (url.get("href") in seen_urls or seen_urls.add(url.get("href")))
   ]
) | scrape_and_summarize.map() | (lambda x: "\n".join(x))

# Set up the search query and agent choice mechanisms using AI models
search_query = SEARCH_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads
choose_agent = CHOOSE_AGENT_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads

# [Initiation] Define how to get search queries based on agent prompts
get_search_queries = {
    "question": lambda x: x,
    "agent_prompt": {"task": lambda x: x} | choose_agent | (lambda x: x["agent_role_prompt"])
} | search_query


class GPTResearcherActor:
    # [Compilation and Output] Define the complete runnable process for the GPT Researcher, compiling all steps
    @property
    def runnable(self):
        return (
            get_search_queries
            | (lambda x: [{"question": q} for q in x])
            | multi_search.map()
            | (lambda x: "\n\n".join(x))
        )


==============================

-search_api.py
from tavily import Client
import os
from langchain.schema.runnable import RunnableLambda


class TavilySearchActor:
    def __init__(self):
        self.api_key = os.environ["TAVILY_API_KEY"]

    @property
    def runnable(self):
        client = Client(self.api_key)
        return RunnableLambda(client.advanced_search) | {"results": lambda x: x["results"]}


==============================

-test.py
# main
import os, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from permchain_example.researcher import Researcher
from permchain_example.search_actors.search_api import TavilySearchActor
from permchain_example.editor_actors.editor import EditorActor
from permchain_example.reviser_actors.reviser import ReviserActor
from permchain_example.search_actors.gpt_researcher import GPTResearcherActor
from permchain_example.writer_actors.writer import WriterActor
from permchain_example.research_team import ResearchTeam
from processing.text import md_to_pdf



if __name__ == '__main__':
    output_path = "./output"
    if not os.path.exists(output_path):
        # If the directory does not exist, create it
        os.makedirs(output_path)

    stocks = ["NVDA"]

    for stock in stocks[:1]:
        query = f"is the stock {stock} a good buy?"
        researcher = Researcher(GPTResearcherActor(), WriterActor())
        research_team = ResearchTeam(researcher, EditorActor(), ReviserActor())

        draft = research_team.run(query)
        with open(f"{output_path}/{stock}.md", "w") as f:
            f.write(draft)
        md_to_pdf(f"{output_path}/{stock}.md", f"{output_path}/{stock}.pdf")

==============================

-writer.py
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from agent.prompts import generate_report_prompt, generate_agent_role_prompt
from config import Config

CFG = Config()

class WriterActor:
    def __init__(self):
        print("WRITER Initializing WriterActor...")
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", generate_agent_role_prompt(agent="Default Agent")),
            ("user", generate_report_prompt(question="{query}", research_summary="{results}"))
        ])

    @property
    def runnable(self):
        return {
            "answer": {
                "query": lambda x: x["query"],
                "results": lambda x: "\n\n".join(x["results"])
              } | self.prompt | self.model | StrOutputParser()
        }


==============================

-__init__.py


==============================

-html.py
"""HTML processing functions"""
from __future__ import annotations

from bs4 import BeautifulSoup
from requests.compat import urljoin


def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """
    return [
        (link.text, urljoin(base_url, link["href"]))
        for link in soup.find_all("a", href=True)
    ]


def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """
    return [f"{link_text} ({link_url})" for link_text, link_url in hyperlinks]


==============================

-text.py
"""Text processing functions"""
import urllib
from typing import Dict, Generator, Optional
import string

from selenium.webdriver.remote.webdriver import WebDriver

from config import Config
from agent.llm_utils import create_chat_completion
import os
from md2pdf.core import md2pdf

CFG = Config()


def split_text(text: str, max_length: int = 8192) -> Generator[str, None, None]:
    """Split text into chunks of a maximum length

    Args:
        text (str): The text to split
        max_length (int, optional): The maximum length of each chunk. Defaults to 8192.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: If the text is longer than the maximum length
    """
    paragraphs = text.split("\n")
    current_length = 0
    current_chunk = []

    for paragraph in paragraphs:
        if current_length + len(paragraph) + 1 <= max_length:
            current_chunk.append(paragraph)
            current_length += len(paragraph) + 1
        else:
            yield "\n".join(current_chunk)
            current_chunk = [paragraph]
            current_length = len(paragraph) + 1

    if current_chunk:
        yield "\n".join(current_chunk)


def summarize_text(
    url: str, text: str, question: str, driver: Optional[WebDriver] = None
) -> str:
    """Summarize text using the OpenAI API

    Args:
        url (str): The url of the text
        text (str): The text to summarize
        question (str): The question to ask the model
        driver (WebDriver): The webdriver to use to scroll the page

    Returns:
        str: The summary of the text
    """
    if not text:
        return "Error: No text to summarize"

    summaries = []
    chunks = list(split_text(text))
    scroll_ratio = 1 / len(chunks)

    print(f"Summarizing url: {url} with total chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        if driver:
            scroll_to_percentage(driver, scroll_ratio * i)

        #memory_to_add = f"Source: {url}\n" f"Raw content part#{i + 1}: {chunk}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

        messages = [create_message(chunk, question)]

        summary = create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
            max_tokens=CFG.summary_token_limit
        )
        summaries.append(summary)
        #memory_to_add = f"Source: {url}\n" f"Content summary part#{i + 1}: {summary}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

    combined_summary = "\n".join(summaries)
    messages = [create_message(combined_summary, question)]

    final_summary = create_chat_completion(
        model=CFG.fast_llm_model,
        messages=messages,
        max_tokens=CFG.summary_token_limit
    )
    print("Final summary length: ", len(combined_summary))
    print(final_summary)

    return final_summary


def scroll_to_percentage(driver: WebDriver, ratio: float) -> None:
    """Scroll to a percentage of the page

    Args:
        driver (WebDriver): The webdriver to use
        ratio (float): The percentage to scroll to

    Raises:
        ValueError: If the ratio is not between 0 and 1
    """
    if ratio < 0 or ratio > 1:
        raise ValueError("Percentage should be between 0 and 1")
    driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {ratio});")


def create_message(chunk: str, question: str) -> Dict[str, str]:
    """Create a message for the chat completion

    Args:
        chunk (str): The chunk of text to summarize
        question (str): The question to answer

    Returns:
        Dict[str, str]: The message to send to the chat completion
    """
    return {
        "role": "user",
        "content": f'"""{chunk}""" Using the above text, answer in short the following'
        f' question: "{question}" -- if the question cannot be answered using the text,'
        " simply summarize the text. "
        "Include all factual information, numbers, stats etc if available.",
    }

def write_to_file(filename: str, text: str) -> None:
    """Write text to a file

    Args:
        text (str): The text to write
        filename (str): The filename to write to
    """
    with open(filename, "w") as file:
        file.write(text)

async def write_md_to_pdf(task: str, path: str, text: str) -> None:
    file_path = f"{path}/{task}"
    write_to_file(f"{file_path}.md", text)
    md_to_pdf(f"{file_path}.md", f"{file_path}.pdf")
    print(f"{task} written to {file_path}.pdf")

    encoded_file_path = urllib.parse.quote(f"{file_path}.pdf")

    return encoded_file_path

def read_txt_files(directory):
    all_text = ''

    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r') as file:
                all_text += file.read() + '\n'

    return all_text


def md_to_pdf(input_file, output_file):
    md2pdf(output_file,
           md_content=None,
           md_file_path=input_file,
           css_file_path=None,
           base_url=None)


==============================

-requirements.txt
# dependencies
asyncio==3.4.3
beautifulsoup4==4.12.2
colorama==0.4.6
duckduckgo_search==3.9.3
md2pdf==1.0.1
openai~=0.28.1
playwright==1.38.0
python-dotenv~=1.0.0
pyyaml==6.0.1
selenium
webdriver-manager==4.0.1
flask
uvicorn
pydantic
fastapi
python-multipart
markdown
langchain==0.0.308
tavily-python
permchain==0.0.3


==============================

-structure.py
import os
import shutil
from pathlib import Path

root_dir = "."  # replace this with your directory path
out_file = "structure.txt"  # rename the output file to structure.txt
extensions = [
    ".js",
    ".handlebars",
    ".json",
    ".py",
    ".txt",
]  # file extensions to collect content from
files_to_copy = [
    "web_scrape.py",
    "web_search.py",
    "llm_utils.py",
    "prompts.py",
    "research_agent.py",
    "run.py",
    "__init__.py,",
    "config.py",
    "singleton.py",
    "editor.py",
    "reviser.py",
    "gpt_researcher.py",
    "search_api.py",
    "writer.py",
    "research_team.py",
    "researcher.py",
    "test.py",
    "html.py",
    "text.py",
    "main.py",
]  # list of filenames to copy

destination_folder = "FOLDER"  # Name of the folder where files will be copied to
if not os.path.exists(destination_folder):
    os.makedirs(destination_folder)


def is_excluded(directory, path):
    """Check if a path should be excluded from processing."""
    exclusions = ["node_modules", ".git", "package-lock.json", "outputs"]
    full_path = os.path.join(directory, path)
    for exclusion in exclusions:
        if full_path.endswith(exclusion):
            return True
    return False


def print_tree(directory, file_output, indent=""):
    if is_excluded(directory, ""):
        return
    contents = sorted(os.listdir(directory))
    pointers = ["â”œâ”€â”€ " if item != contents[-1] else "â””â”€â”€ " for item in contents]
    for pointer, path in zip(pointers, contents):
        if is_excluded(directory, path):
            continue
        full_path = os.path.join(directory, path)
        print(indent + pointer + path, file=file_output)
        if os.path.isdir(full_path):
            next_indent = "â”‚   " if pointer.startswith("â”œ") else "    "
            print_tree(full_path, file_output, indent=indent + next_indent)


def print_file_contents(directory, file_output, indent=""):
    if is_excluded(directory, ""):
        return
    contents = sorted(os.listdir(directory))
    for path in contents:
        if is_excluded(directory, path):
            continue
        full_path = os.path.join(directory, path)
        if os.path.isfile(full_path) and Path(full_path).suffix in extensions:
            print("\n" + "=" * 30 + "\n", file=file_output)  # separator line
            print("-" + Path(full_path).name, file=file_output)
            with open(full_path, "r") as content_file:
                print(content_file.read(), file=file_output)
        elif os.path.isdir(full_path):
            print_file_contents(full_path, file_output, indent=indent + "    ")


def copy_files(directory):
    """Search and copy the specific files to the destination folder."""
    contents = sorted(os.listdir(directory))
    for path in contents:
        if is_excluded(directory, path):
            continue
        full_path = os.path.join(directory, path)
        if os.path.isfile(full_path) and path in files_to_copy:
            shutil.copy(full_path, destination_folder)
        elif os.path.isdir(full_path):
            copy_files(full_path)


copy_files(root_dir)

with open(out_file, "w", encoding="utf-8") as f:  # specify utf-8 encoding
    print_tree(root_dir, f)
    print_file_contents(root_dir, f)


==============================

-structure.txt
â”œâ”€â”€ .DS_Store
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .github
â”‚   â”œâ”€â”€ dependabot.yml
â”‚   â””â”€â”€ workflows
â”‚       â””â”€â”€ docker-bulid.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .vscode
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ CODE_OF_CONDUCT.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ FOLDER
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ editor.py
â”‚   â”œâ”€â”€ gpt_researcher.py
â”‚   â”œâ”€â”€ html.py
â”‚   â”œâ”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ research_agent.py
â”‚   â”œâ”€â”€ research_team.py
â”‚   â”œâ”€â”€ researcher.py
â”‚   â”œâ”€â”€ reviser.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ search_api.py
â”‚   â”œâ”€â”€ singleton.py
â”‚   â”œâ”€â”€ test.py
â”‚   â”œâ”€â”€ text.py
â”‚   â”œâ”€â”€ web_scrape.py
â”‚   â”œâ”€â”€ web_search.py
â”‚   â””â”€â”€ writer.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Letter-to-USPTO-USCO-on-National-Commission-on-AI-1.pdf.crdownload
â”œâ”€â”€ National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf.crdownload
â”œâ”€â”€ README.md
â”œâ”€â”€ The Origin of LL.M. Programs_ A Case Study of the University of P.pdf.crdownload
â”œâ”€â”€ __pycache__
â”‚   â””â”€â”€ main.cpython-311.pyc
â”œâ”€â”€ actions
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ web_scrape.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ web_search.cpython-311.pyc
â”‚   â”œâ”€â”€ web_scrape.py
â”‚   â””â”€â”€ web_search.py
â”œâ”€â”€ agent
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ llm_utils.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ prompts.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ research_agent.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ run.cpython-311.pyc
â”‚   â”œâ”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ research_agent.py
â”‚   â””â”€â”€ run.py
â”œâ”€â”€ avip.pdf.crdownload
â”œâ”€â”€ client
â”‚   â”œâ”€â”€ .DS_Store
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ scripts.js
â”‚   â”œâ”€â”€ static
â”‚   â”‚   â”œâ”€â”€ academicResearchAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ businessAnalystAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ computerSecurityanalystAvatar.png
â”‚   â”‚   â”œâ”€â”€ defaultAgentAvatar2.JPG
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â”œâ”€â”€ financeAgentAvatar.png
â”‚   â”‚   â”œâ”€â”€ hulk.png
â”‚   â”‚   â”œâ”€â”€ mathAgentAvatar.png
â”‚   â”‚   â””â”€â”€ travelAgentAvatar.png
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ config.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ singleton.cpython-311.pyc
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ singleton.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ grain.pdf.crdownload
â”œâ”€â”€ js
â”‚   â””â”€â”€ overlay.js
â”œâ”€â”€ main.py
â”œâ”€â”€ p17.pdf.crdownload
â”œâ”€â”€ permchain_example
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ editor_actors
â”‚   â”‚   â””â”€â”€ editor.py
â”‚   â”œâ”€â”€ research_team.py
â”‚   â”œâ”€â”€ researcher.py
â”‚   â”œâ”€â”€ reviser_actors
â”‚   â”‚   â””â”€â”€ reviser.py
â”‚   â”œâ”€â”€ search_actors
â”‚   â”‚   â”œâ”€â”€ gpt_researcher.py
â”‚   â”‚   â””â”€â”€ search_api.py
â”‚   â”œâ”€â”€ test.py
â”‚   â””â”€â”€ writer_actors
â”‚       â””â”€â”€ writer.py
â”œâ”€â”€ processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-311.pyc
â”‚   â”‚   â”œâ”€â”€ html.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ text.cpython-311.pyc
â”‚   â”œâ”€â”€ html.py
â”‚   â””â”€â”€ text.py
â”œâ”€â”€ prompts.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ structure.py
â””â”€â”€ structure.txt

==============================

-settings.json
{
  "[python]": {
    "editor.defaultFormatter": "ms-python.black-formatter"
  },
  "python.formatting.provider": "none"
}


==============================

-config.py
"""Configuration class to store the state of bools for different scripts access."""
import os

import openai
from colorama import Fore
from dotenv import load_dotenv

from config.singleton import Singleton

load_dotenv(verbose=True)


class Config(metaclass=Singleton):
    """
    Configuration class to store the state of bools for different scripts access.
    """

    def __init__(self) -> None:
        """Initialize the Config class"""
        self.debug_mode = False
        self.allow_downloads = False

        self.selenium_web_browser = os.getenv("USE_WEB_BROWSER", "chrome")
        self.llm_provider = os.getenv("LLM_PROVIDER", "ChatOpenAI")
        self.fast_llm_model = os.getenv("FAST_LLM_MODEL", "gpt-3.5-turbo-16k")
        self.smart_llm_model = os.getenv("SMART_LLM_MODEL", "gpt-4")
        self.fast_token_limit = int(os.getenv("FAST_TOKEN_LIMIT", 2000))
        self.smart_token_limit = int(os.getenv("SMART_TOKEN_LIMIT", 4000))
        self.browse_chunk_max_length = int(os.getenv("BROWSE_CHUNK_MAX_LENGTH", 8192))
        self.summary_token_limit = int(os.getenv("SUMMARY_TOKEN_LIMIT", 700))

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.temperature = float(os.getenv("TEMPERATURE", "1"))

        self.user_agent = os.getenv(
            "USER_AGENT",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36"
            " (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36",
        )

        self.memory_backend = os.getenv("MEMORY_BACKEND", "local")
        # Initialize the OpenAI API client
        openai.api_key = self.openai_api_key

    def set_fast_llm_model(self, value: str) -> None:
        """Set the fast LLM model value."""
        self.fast_llm_model = value

    def set_smart_llm_model(self, value: str) -> None:
        """Set the smart LLM model value."""
        self.smart_llm_model = value

    def set_fast_token_limit(self, value: int) -> None:
        """Set the fast token limit value."""
        self.fast_token_limit = value

    def set_smart_token_limit(self, value: int) -> None:
        """Set the smart token limit value."""
        self.smart_token_limit = value

    def set_browse_chunk_max_length(self, value: int) -> None:
        """Set the browse_website command chunk max length value."""
        self.browse_chunk_max_length = value

    def set_openai_api_key(self, value: str) -> None:
        """Set the OpenAI API key value."""
        self.openai_api_key = value

    def set_debug_mode(self, value: bool) -> None:
        """Set the debug mode value."""
        self.debug_mode = value


def check_openai_api_key() -> None:
    """Check if the OpenAI API key is set in config.py or as an environment variable."""
    cfg = Config()
    if not cfg.openai_api_key:
        print(
            Fore.RED
            + "Please set your OpenAI API key in .env or as an environment variable."
        )
        print("You can get your key from https://platform.openai.com/account/api-keys")
        exit(1)


==============================

-editor.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

EDIT_TEMPLATE = """You are an editor. \
You have been tasked with editing the following draft, which was written by a non-expert. \
Please accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision. \
Things you should be checking for:

- This draft MUST fully answer the original question
- This draft MUST be written in apa format

If not all of the above criteria are met, you should send appropriate revision notes.
"""


class EditorActor:
    def __init__(self):
        print("EDITOR Initializing EditorActor")  # Indicates the start of the EditorActor initialization

        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(EDIT_TEMPLATE) + "Draft:\n\n{draft}"
        self.functions = [
            {
                "name": "revise",
                "description": "Sends the draft for revision",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "notes": {
                            "type": "string",
                            "description": "The editor's notes to guide the revision.",
                        },
                    },
                },
            },
            {
                "name": "accept",
                "description": "Accepts the draft",
                "parameters": {
                    "type": "object",
                    "properties": {"ready": {"const": True}},
                },
            },
        ]

    @property
    def runnable(self):
        return (
            self.prompt | self.model.bind(functions=self.functions)
        )


==============================

-gpt_researcher.py
import json
# Import necessary modules for text processing, web scraping, and searching
from processing.text import summarize_text
from actions.web_scrape import scrape_text_with_selenium
from actions.web_search import web_search

# Import Langchain and related utilities for AI-driven chat and prompt management
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableMap, RunnableLambda
from langchain.schema.messages import SystemMessage
from agent.prompts import auto_agent_instructions, generate_search_queries_prompt
from config import Config

# Load global configurations
CFG = Config()

# [Initiation] Prepare the AI-driven message template for generating search queries
search_message = (generate_search_queries_prompt("{question}"))
SEARCH_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "{agent_prompt}"),
    ("user", search_message)
])

# Load instructions for automated agent behavior
AUTO_AGENT_INSTRUCTIONS = auto_agent_instructions()
CHOOSE_AGENT_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessage(content=AUTO_AGENT_INSTRUCTIONS),
    ("user", "task: {task}")
])

# [Content Retrieval and Summarization and Analysis] Define the process for scraping and summarizing text from a URL
scrape_and_summarize = {
    "question": lambda x: x["question"],
    "text": lambda x: scrape_text_with_selenium(x['url'])[1],
    "url": lambda x: x['url']
} | RunnableMap({
        "summary": lambda x: summarize_text(text=x["text"], question=x["question"], url=x["url"]),
        "url": lambda x: x['url']
}) | (lambda x: f"Source Url: {x['url']}\nSummary: {x['summary']}")

# Initialize a set to keep track of URLs that have already been seen to avoid duplicate content
seen_urls = set()

# [Web Search and Content Retrieval] Define the process for conducting multiple searches, avoiding duplicate URLs, and processing the results
multi_search = (
    lambda x: [
        {"url": url.get("href"), "question": x["question"]}
        for url in json.loads(web_search(query=x["question"], num_results=3))
        if not (url.get("href") in seen_urls or seen_urls.add(url.get("href")))
   ]
) | scrape_and_summarize.map() | (lambda x: "\n".join(x))

# Set up the search query and agent choice mechanisms using AI models
search_query = SEARCH_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads
choose_agent = CHOOSE_AGENT_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads

# [Initiation] Define how to get search queries based on agent prompts
get_search_queries = {
    "question": lambda x: x,
    "agent_prompt": {"task": lambda x: x} | choose_agent | (lambda x: x["agent_role_prompt"])
} | search_query


class GPTResearcherActor:
    # [Compilation and Output] Define the complete runnable process for the GPT Researcher, compiling all steps
    @property
    def runnable(self):
        return (
            get_search_queries
            | (lambda x: [{"question": q} for q in x])
            | multi_search.map()
            | (lambda x: "\n\n".join(x))
        )


==============================

-html.py
"""HTML processing functions"""
from __future__ import annotations

from bs4 import BeautifulSoup
from requests.compat import urljoin


def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """
    return [
        (link.text, urljoin(base_url, link["href"]))
        for link in soup.find_all("a", href=True)
    ]


def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """
    return [f"{link_text} ({link_url})" for link_text, link_url in hyperlinks]


==============================

-llm_utils.py
from __future__ import annotations

import json

from fastapi import WebSocket
import time

import openai
from langchain.adapters import openai as lc_openai
from colorama import Fore, Style
from openai.error import APIError, RateLimitError

from agent.prompts import auto_agent_instructions
from config import Config

CFG = Config()

openai.api_key = CFG.openai_api_key

from typing import Optional
import logging


def create_chat_completion(
    messages: list,  # type: ignore
    model: Optional[str] = None,
    temperature: float = CFG.temperature,
    max_tokens: Optional[int] = None,
    stream: Optional[bool] = False,
    websocket: WebSocket | None = None,
) -> str:
    """Create a chat completion using the OpenAI API
    Args:
        messages (list[dict[str, str]]): The messages to send to the chat completion
        model (str, optional): The model to use. Defaults to None.
        temperature (float, optional): The temperature to use. Defaults to 0.9.
        max_tokens (int, optional): The max tokens to use. Defaults to None.
        stream (bool, optional): Whether to stream the response. Defaults to False.
    Returns:
        str: The response from the chat completion
    """

    # validate input
    if model is None:
        raise ValueError("Model cannot be None")
    if max_tokens is not None and max_tokens > 8001:
        raise ValueError(f"Max tokens cannot be more than 8001, but got {max_tokens}")
    if stream and websocket is None:
        raise ValueError("Websocket cannot be None when stream is True")

    # create response
    for attempt in range(10):  # maximum of 10 attempts
        response = send_chat_completion_request(
            messages, model, temperature, max_tokens, stream, websocket
        )
        return response

    logging.error("Failed to get response from OpenAI API")
    raise RuntimeError("Failed to get response from OpenAI API")


def send_chat_completion_request(
    messages, model, temperature, max_tokens, stream, websocket
):
    if not stream:
        result = lc_openai.ChatCompletion.create(
            model=model,  # Change model here to use different models
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            provider=CFG.llm_provider,  # Change provider here to use a different API
        )
        return result["choices"][0]["message"]["content"]
    else:
        return stream_response(model, messages, temperature, max_tokens, websocket)


async def stream_response(model, messages, temperature, max_tokens, websocket):
    paragraph = ""
    response = ""
    print(f"LLM_UTILS streaming response...")

    for chunk in lc_openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        provider=CFG.llm_provider,
        stream=True,
    ):
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content is not None:
            response += content
            paragraph += content
            if "\n" in paragraph:
                await websocket.send_json({"type": "report", "output": paragraph})
                paragraph = ""
    print(f"LLM_UTILS streaming response complete")
    return response


def choose_agent(task: str) -> dict:
    """Determines what agent should be used
    Args:
        task (str): The research question the user asked
    Returns:
        agent - The agent that will be used
        agent_role_prompt (str): The prompt for the agent
    """
    try:
        response = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{auto_agent_instructions()}"},
                {"role": "user", "content": f"task: {task}"},
            ],
            temperature=0,
        )

        return json.loads(response)
    except Exception as e:
        print(f"{Fore.RED}Error in choose_agent: {e}{Style.RESET_ALL}")
        return {
            "agent": "Default Agent",
            "agent_role_prompt": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
        }


==============================

-main.py
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import os

from agent.llm_utils import choose_agent
from agent.run import WebSocketManager


class ResearchRequest(BaseModel):
    task: str
    report_type: str
    agent: str



app = FastAPI()
app.mount("/site", StaticFiles(directory="client"), name="site")
app.mount("/static", StaticFiles(directory="client/static"), name="static")
# Dynamic directory for outputs once first research is run
@app.on_event("startup")
def startup_event():
    if not os.path.isdir("outputs"):
        os.makedirs("outputs")
    app.mount("/outputs", StaticFiles(directory="outputs"), name="outputs")

templates = Jinja2Templates(directory="client")

manager = WebSocketManager()


@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse('index.html', {"request": request, "report": None})


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            if data.startswith("start"):
                json_data = json.loads(data[6:])
                task = json_data.get("task")
                report_type = json_data.get("report_type")
                agent = json_data.get("agent")
                # temporary so "normal agents" can still be used and not just auto generated, will be removed when we move to auto generated
                if agent == "Auto Agent":
                    agent_dict = choose_agent(task)
                    agent = agent_dict.get("agent")
                    agent_role_prompt = agent_dict.get("agent_role_prompt")
                else:
                    agent_role_prompt = None

                await websocket.send_json({"type": "logs", "output": f"Initiated an Agent: {agent}"})
                if task and report_type and agent:
                    await manager.start_streaming(task, report_type, agent, agent_role_prompt, websocket)
                else:
                    print("Error: not enough parameters provided.")

    except WebSocketDisconnect:
        await manager.disconnect(websocket)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)


==============================

-prompts.py
from datetime import datetime


def generate_agent_role_prompt(agent):
    """Generates the agent role prompt.
    Args: agent (str): The type of the agent.
    Returns: str: The agent role prompt.
    """
    prompts = {
        "Finance Agent": "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.",
        "Travel Agent": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.",
        "Academic Research Agent": "You are an AI academic research assistant. Your primary responsibility is to create thorough, academically rigorous, unbiased, and systematically organized reports on a given research topic, following the standards of scholarly work.",
        "Business Analyst": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.",
        "Computer Security Analyst Agent": "You are an AI specializing in computer security analysis. Your principal duty is to generate comprehensive, meticulously detailed, impartial, and systematically structured reports on computer security topics. This includes Exploits, Techniques, Threat Actors, and Advanced Persistent Threat (APT) Groups. All produced reports should adhere to the highest standards of scholarly work and provide in-depth insights into the complexities of computer security.",
        "Default Agent": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
    }

    return prompts.get(agent, "No such agent")


def generate_report_prompt(question, research_summary):
    """Generates the report prompt for the given question and research summary.
    Args: question (str): The question to generate the report prompt for
            research_summary (str): The research summary to generate the report prompt for
    Returns: str: The report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, answer the following'
        f' question or topic: "{question}" in a detailed report --'
        " The report should focus on the answer to the question, should be well structured, informative,"
        " in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format.\n "
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n"
        f"Write all used source urls at the end of the report in apa format.\n "
        f"Assume that the current date is {datetime.now().strftime('%B %d, %Y')}"
    )


def generate_search_queries_prompt(question):
    """Generates the search queries prompt for the given question.
    Args: question (str): The question to generate the search queries prompt for
    Returns: str: The search queries prompt for the given question
    """
    print(
        f"PROMPTS Generating search queries prompt for: {question}"
    )  # Indicates the specific question being processed for search queries

    queries = (
        f'Write 3 google search queries to search online that form an objective opinion from the following: "{question}"'
        f'Use the current date if needed: {datetime.now().strftime("%B %d, %Y")}.\n'
        f'You must respond with a list of strings in the following format: ["query 1", "query 2", "query 3"].'
    )
    print(f"PROMPTS Generated search queries: {queries}")  # Print the queries

    return queries


def generate_resource_report_prompt(question, research_summary):
    """Generates the resource report prompt for the given question and research summary.

    Args:
        question (str): The question to generate the resource report prompt for.
        research_summary (str): The research summary to generate the resource report prompt for.

    Returns:
        str: The resource report prompt for the given question and research summary.
    """
    return (
        f'"""{research_summary}""" Based on the above information, generate a bibliography recommendation report for the following'
        f' question or topic: "{question}". The report should provide a detailed analysis of each recommended resource,'
        " explaining how each source can contribute to finding answers to the research question."
        " Focus on the relevance, reliability, and significance of each source."
        " Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax."
        " Include relevant facts, figures, and numbers whenever available."
        " The report should have a minimum length of 1,200 words."
    )


def generate_outline_report_prompt(question, research_summary):
    """Generates the outline report prompt for the given question and research summary.
    Args: question (str): The question to generate the outline report prompt for
            research_summary (str): The research summary to generate the outline report prompt for
    Returns: str: The outline report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, generate an outline for a research report in Markdown syntax'
        f' for the following question or topic: "{question}". The outline should provide a well-structured framework'
        " for the research report, including the main sections, subsections, and key points to be covered."
        " The research report should be detailed, informative, in-depth, and a minimum of 1,200 words."
        " Use appropriate Markdown syntax to format the outline and ensure readability."
    )


def generate_concepts_prompt(question, research_summary):
    """Generates the concepts prompt for the given question.
    Args: question (str): The question to generate the concepts prompt for
            research_summary (str): The research summary to generate the concepts prompt for
    Returns: str: The concepts prompt for the given question
    """

    return (
        f'"""{research_summary}""" Using the above information, generate a list of 5 main concepts to learn for a research report'
        f' on the following question or topic: "{question}". The outline should provide a well-structured framework'
        'You must respond with a list of strings in the following format: ["concepts 1", "concepts 2", "concepts 3", "concepts 4, concepts 5"]'
    )


def generate_lesson_prompt(concept):
    """
    Generates the lesson prompt for the given question.
    Args:
        concept (str): The concept to generate the lesson prompt for.
    Returns:
        str: The lesson prompt for the given concept.
    """

    prompt = (
        f"generate a comprehensive lesson about {concept} in Markdown syntax. This should include the definition"
        f"of {concept}, its historical background and development, its applications or uses in different"
        f"fields, and notable events or facts related to {concept}."
    )

    return prompt


def get_report_by_type(report_type):
    report_type_mapping = {
        "research_report": generate_report_prompt,
        "resource_report": generate_resource_report_prompt,
        "outline_report": generate_outline_report_prompt,
    }
    return report_type_mapping[report_type]


def auto_agent_instructions():
    return """
        This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific agent, defined by its type and role, with each agent requiring distinct instructions.
        Agent
        The agent is determined by the field of the topic and the specific name of the agent that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each agent type is associated with a corresponding emoji.

        examples:
        task: "should I invest in apple stocks?"
        response: 
        {
            "agent": "ðŸ’° Finance Agent",
            "agent_role_prompt: "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends."
        }
        task: "could reselling sneakers become profitable?"
        response: 
        { 
            "agent":  "ðŸ“ˆ Business Analyst Agent",
            "agent_role_prompt": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis."
        }
        task: "what are the most interesting sites in Tel Aviv?"
        response:
        {
            "agent:  "ðŸŒ Travel Agent",
            "agent_role_prompt": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights."
        }
    """


==============================

-research_agent.py
# Description: Research assistant class that handles the research process for a given question.

# libraries
import asyncio
import json
import hashlib
from actions.web_search import web_search
from actions.web_scrape import async_browse
from processing.text import (
    write_to_file,
    create_message,
    create_chat_completion,
    read_txt_files,
    write_md_to_pdf,
)
from config import Config
from agent import prompts
import os
import string

CFG = Config()


class ResearchAgent:
    def __init__(self, question, agent, agent_role_prompt, websocket=None):
        """Initializes the research assistant with the given question.
        Args: question (str): The question to research
        Returns: None
        """
        self.question = question
        self.agent = agent
        self.agent_role_prompt = (
            agent_role_prompt
            if agent_role_prompt
            else prompts.generate_agent_role_prompt(agent)
        )
        self.visited_urls = set()
        self.research_summary = ""
        self.dir_path = f"./outputs/{hashlib.sha1(question.encode()).hexdigest()}"
        self.websocket = websocket

        print("RESEARCH_AGENT ResearchAgent initialized with question:", self.question)

    async def stream_output(self, output):
        if not self.websocket:
            return print(output)
        await self.websocket.send_json({"type": "logs", "output": output})

    async def summarize(self, text, topic):
        """Summarizes the given text for the given topic.
        Args: text (str): The text to summarize
                topic (str): The topic to summarize the text for
        Returns: str: The summarized text
        """
        print("RESEARCH_AGENT Starting summarization for topic:", topic)

        messages = [create_message(text, topic)]
        await self.stream_output(f"ðŸ“ Summarizing text for query: {text}")

        return create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
        )

    async def get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """
        new_urls = []
        for url in url_set_input:
            if url not in self.visited_urls:
                await self.stream_output(f"âœ… Adding source url to research: {url}\n")

                self.visited_urls.add(url)
                new_urls.append(url)

        return new_urls

    async def call_agent(self, action, stream=False, websocket=None):
        messages = [
            {"role": "system", "content": self.agent_role_prompt},
            {
                "role": "user",
                "content": action,
            },
        ]
        answer = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=messages,
            stream=stream,
            websocket=websocket,
        )
        return answer

    async def create_search_queries(self):
        """Modifies the method to use the user's input directly as the search query.
        Args: None
        Returns: list[str]: The search queries for the given question
        """
        # Directly use the user's input (self.question) as the search query
        await self.stream_output(
            f"ðŸ§  I will conduct my research based on the following query: '{self.question}'..."
        )
        return [self.question]

    async def async_search(self, query):
        """Runs the async search for the given query.
        Args: query (str): The query to run the async search for
        Returns: list[str]: The async search for the given query
        """
        search_results = json.loads(web_search(query))
        new_search_urls = await self.get_new_urls([url.get("link") for url in search_results])

        await self.stream_output(
            f"ðŸŒ Browsing the following sites for relevant information: {new_search_urls}..."
        )

        # Create a list to hold the coroutine objects
        tasks = [
            async_browse(url, query, self.websocket) for url in new_search_urls
        ]

        # Gather the results as they become available
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        return responses

    async def run_search_summary(self, query):
        """Runs the search summary for the given query.
        Args: query (str): The query to run the search summary for
        Returns: str: The search summary for the given query
        """
        await self.stream_output(f"ðŸ”Ž Running research for '{query}'...")

        responses = await self.async_search(query)

        result = "\n".join(responses)
        os.makedirs(
            os.path.dirname(f"{self.dir_path}/research-{query}.txt"), exist_ok=True
        )
        write_to_file(f"{self.dir_path}/research-{query}.txt", result)
        return result

    async def conduct_research(self):
        """Conducts the research for the given question.
        Args: None
        Returns: str: The research for the given question
        """
        self.research_summary = (
            read_txt_files(self.dir_path) if os.path.isdir(self.dir_path) else ""
        )

        if not self.research_summary:
            search_queries = await self.create_search_queries()
            for query in search_queries:
                research_result = await self.run_search_summary(query)
                self.research_summary += f"{research_result}\n\n"

        await self.stream_output(
            f"Total research words: {len(self.research_summary.split(' '))}"
        )

        return self.research_summary

    async def create_concepts(self):
        """Creates the concepts for the given question.
        Args: None
        Returns: list[str]: The concepts for the given question
        """
        result = await self.call_agent(
            prompts.generate_concepts_prompt(self.question, self.research_summary)
        )

        await self.stream_output(
            f"I will research based on the following concepts: {result}\n"
        )
        return json.loads(result)

    async def write_report(self, report_type, websocket=None):
        """Writes the report for the given question.
        Args: None
        Returns: str: The report for the given question
        """
        print("RESEARCH_AGENT Starting to write report of type:", report_type)

        report_type_func = prompts.get_report_by_type(report_type)
        await self.stream_output(
            f"âœï¸ Writing {report_type} for research task: {self.question}..."
        )

        answer = await self.call_agent(
            report_type_func(self.question, self.research_summary),
            stream=websocket is not None,
            websocket=websocket,
        )
        # if websocket is True than we are streaming gpt response, so we need to wait for the final response
        final_report = await answer if websocket else answer

        path = await write_md_to_pdf(report_type, self.dir_path, final_report)

        return answer, path

    async def write_lessons(self):
        """Writes lessons on essential concepts of the research.
        Args: None
        Returns: None
        """
        concepts = await self.create_concepts()
        for concept in concepts:
            answer = await self.call_agent(
                prompts.generate_lesson_prompt(concept), stream=True
            )
            await write_md_to_pdf("Lesson", self.dir_path, answer)


==============================

-research_team.py
from operator import itemgetter
from langchain.runnables.openai_functions import OpenAIFunctionsRouter

from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic

"""
    This is the research team. 
    It is a group of autonomous agents that work together to answer a given question
    using a comprehensive research process that includes: 
        - Searching for relevant information across multiple sources
        - Extracting relevant information
        - Writing a well structured report
        - Validating the report
        - Revising the report
        - Repeat until the report is satisfactory
"""


class ResearchTeam:
    def __init__(self, research_actor, editor_actor, reviser_actor):
        self.research_actor_instance = research_actor
        self.editor_actor_instance = editor_actor
        self.revise_actor_instance = reviser_actor

    def run(self, query):
        print("ResearchTeam: Initialized with query:", query)  # Starting the research process

        # create topics
        editor_inbox = Topic("editor_inbox")
        reviser_inbox = Topic("reviser_inbox")
        print("ResearchTeam: Topics created - editor_inbox, reviser_inbox")

        research_chain = (
            # Listed in inputs
            Topic.IN.subscribe()
            | {"draft": lambda x: self.research_actor_instance.run(x["question"])}
            # The draft always goes to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Research chain initialized")

        editor_chain = (
            # Listen for events in the editor_inbox
            editor_inbox.subscribe()
            | self.editor_actor_instance.runnable
            # Depending on the output, different things should happen
            | OpenAIFunctionsRouter(
                {
                    # If revise is chosen, we send a push to the critique_inbox
                    "revise": (
                        {
                            "notes": itemgetter("notes"),
                            "draft": editor_inbox.current() | itemgetter("draft"),
                            "question": Topic.IN.current() | itemgetter("question"),
                        }
                        | reviser_inbox.publish()
                    ),
                    # If accepted, then we return
                    "accept": editor_inbox.current() | Topic.OUT.publish(),
                }
            )
        )
        print("ResearchTeam: Editor chain initialized")

        reviser_chain = (
            # Listen for events in the reviser's inbox
            reviser_inbox.subscribe()
            | self.revise_actor_instance.runnable
            # Publish to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Reviser chain initialized")

        web_researcher = PubSub(
            research_chain,
            editor_chain,
            reviser_chain,
            connection=InMemoryPubSubConnection(),
        )
        print("ResearchTeam: PubSub initialized with all chains")

        res = web_researcher.invoke({"question": query})
        print("ResearchTeam: Invocation complete with result:", res)

        return res["draft"]


==============================

-researcher.py
from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic


class Researcher:
    def __init__(self, search_actor, writer_actor):
        print("RESEARCHER Initializing Researcher...")
        self.search_actor_instance = search_actor
        self.writer_actor_instance = writer_actor

    def run(self, query):
        # The research inbox
        research_inbox = Topic("research")
        search_actor = (
            Topic.IN.subscribe()
            | {"query": lambda x: x, "results": self.search_actor_instance.runnable}
            | research_inbox.publish()
        )

        write_actor = (
            research_inbox.subscribe()
            | self.writer_actor_instance.runnable
            | Topic.OUT.publish()
        )

        researcher = PubSub(
            search_actor,
            write_actor,
            connection=InMemoryPubSubConnection(),
        )

        res = researcher.invoke(query)
        return res["answer"]


==============================

-reviser.py
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

class ReviserActor:
    def __init__(self):
        print("REVISER Initializing ReviserActor")  # Indicates the start of the EditorActor initialization

        
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(
            "You are an expert writer. "
            "You have been tasked by your editor with revising the following draft, which was written by a non-expert. "
            "You may follow the editor's notes or not, as you see fit."
        ) + "Draft:\n\n{draft}" + "Editor's notes:\n\n{notes}"

    @property
    def runnable(self):
        return {
            "draft": {
                "draft": lambda x: x["draft"],
                "notes": lambda x: x["notes"],
            } | self.prompt | self.model | StrOutputParser()
        }


==============================

-run.py
#manage WebSocket connections and the execution of an agent's research task
import asyncio
import datetime

from typing import List, Dict
from fastapi import WebSocket
from config import check_openai_api_key
from agent.research_agent import ResearchAgent


class WebSocketManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}
        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}

    async def start_sender(self, websocket: WebSocket):
        queue = self.message_queues[websocket]
        while True:
            message = await queue.get()
            if websocket in self.active_connections:
                await websocket.send_text(message)
            else:
                break

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.message_queues[websocket] = asyncio.Queue()
        self.sender_tasks[websocket] = asyncio.create_task(self.start_sender(websocket))

    async def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        self.sender_tasks[websocket].cancel()
        del self.sender_tasks[websocket]
        del self.message_queues[websocket]

    async def start_streaming(self, task, report_type, agent, agent_role_prompt, websocket):
        report, path = await run_agent(task, report_type, agent, agent_role_prompt, websocket)
        return report, path


async def run_agent(task, report_type, agent, agent_role_prompt, websocket):
    check_openai_api_key()

    start_time = datetime.datetime.now()

    # await websocket.send_json({"type": "logs", "output": f"Start time: {str(start_time)}\n\n"})

    assistant = ResearchAgent(task, agent, agent_role_prompt, websocket)
    await assistant.conduct_research()

    report, path = await assistant.write_report(report_type, websocket)

    await websocket.send_json({"type": "path", "output": path})

    end_time = datetime.datetime.now()
    await websocket.send_json({"type": "logs", "output": f"\nEnd time: {end_time}\n"})
    await websocket.send_json({"type": "logs", "output": f"\nTotal run time: {end_time - start_time}\n"})

    return report, path


==============================

-search_api.py
from tavily import Client
import os
from langchain.schema.runnable import RunnableLambda


class TavilySearchActor:
    def __init__(self):
        self.api_key = os.environ["TAVILY_API_KEY"]

    @property
    def runnable(self):
        client = Client(self.api_key)
        return RunnableLambda(client.advanced_search) | {"results": lambda x: x["results"]}


==============================

-singleton.py
"""The singleton metaclass for ensuring only one instance of a class."""
import abc


class Singleton(abc.ABCMeta, type):
    """
    Singleton metaclass for ensuring only one instance of a class.
    """

    _instances = {}

    def __call__(cls, *args, **kwargs):
        """Call method for the singleton metaclass."""
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class AbstractSingleton(abc.ABC, metaclass=Singleton):
    """
    Abstract singleton class for ensuring only one instance of a class.
    """

    pass


==============================

-test.py
# main
import os, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from permchain_example.researcher import Researcher
from permchain_example.search_actors.search_api import TavilySearchActor
from permchain_example.editor_actors.editor import EditorActor
from permchain_example.reviser_actors.reviser import ReviserActor
from permchain_example.search_actors.gpt_researcher import GPTResearcherActor
from permchain_example.writer_actors.writer import WriterActor
from permchain_example.research_team import ResearchTeam
from processing.text import md_to_pdf



if __name__ == '__main__':
    output_path = "./output"
    if not os.path.exists(output_path):
        # If the directory does not exist, create it
        os.makedirs(output_path)

    stocks = ["NVDA"]

    for stock in stocks[:1]:
        query = f"is the stock {stock} a good buy?"
        researcher = Researcher(GPTResearcherActor(), WriterActor())
        research_team = ResearchTeam(researcher, EditorActor(), ReviserActor())

        draft = research_team.run(query)
        with open(f"{output_path}/{stock}.md", "w") as f:
            f.write(draft)
        md_to_pdf(f"{output_path}/{stock}.md", f"{output_path}/{stock}.pdf")

==============================

-text.py
"""Text processing functions"""
import urllib
from typing import Dict, Generator, Optional
import string

from selenium.webdriver.remote.webdriver import WebDriver

from config import Config
from agent.llm_utils import create_chat_completion
import os
from md2pdf.core import md2pdf

CFG = Config()


def split_text(text: str, max_length: int = 8192) -> Generator[str, None, None]:
    """Split text into chunks of a maximum length

    Args:
        text (str): The text to split
        max_length (int, optional): The maximum length of each chunk. Defaults to 8192.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: If the text is longer than the maximum length
    """
    paragraphs = text.split("\n")
    current_length = 0
    current_chunk = []

    for paragraph in paragraphs:
        if current_length + len(paragraph) + 1 <= max_length:
            current_chunk.append(paragraph)
            current_length += len(paragraph) + 1
        else:
            yield "\n".join(current_chunk)
            current_chunk = [paragraph]
            current_length = len(paragraph) + 1

    if current_chunk:
        yield "\n".join(current_chunk)


def summarize_text(
    url: str, text: str, question: str, driver: Optional[WebDriver] = None
) -> str:
    """Summarize text using the OpenAI API

    Args:
        url (str): The url of the text
        text (str): The text to summarize
        question (str): The question to ask the model
        driver (WebDriver): The webdriver to use to scroll the page

    Returns:
        str: The summary of the text
    """
    if not text:
        return "Error: No text to summarize"

    summaries = []
    chunks = list(split_text(text))
    scroll_ratio = 1 / len(chunks)

    print(f"Summarizing url: {url} with total chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        if driver:
            scroll_to_percentage(driver, scroll_ratio * i)

        #memory_to_add = f"Source: {url}\n" f"Raw content part#{i + 1}: {chunk}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

        messages = [create_message(chunk, question)]

        summary = create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
            max_tokens=CFG.summary_token_limit
        )
        summaries.append(summary)
        #memory_to_add = f"Source: {url}\n" f"Content summary part#{i + 1}: {summary}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

    combined_summary = "\n".join(summaries)
    messages = [create_message(combined_summary, question)]

    final_summary = create_chat_completion(
        model=CFG.fast_llm_model,
        messages=messages,
        max_tokens=CFG.summary_token_limit
    )
    print("Final summary length: ", len(combined_summary))
    print(final_summary)

    return final_summary


def scroll_to_percentage(driver: WebDriver, ratio: float) -> None:
    """Scroll to a percentage of the page

    Args:
        driver (WebDriver): The webdriver to use
        ratio (float): The percentage to scroll to

    Raises:
        ValueError: If the ratio is not between 0 and 1
    """
    if ratio < 0 or ratio > 1:
        raise ValueError("Percentage should be between 0 and 1")
    driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {ratio});")


def create_message(chunk: str, question: str) -> Dict[str, str]:
    """Create a message for the chat completion

    Args:
        chunk (str): The chunk of text to summarize
        question (str): The question to answer

    Returns:
        Dict[str, str]: The message to send to the chat completion
    """
    return {
        "role": "user",
        "content": f'"""{chunk}""" Using the above text, answer in short the following'
        f' question: "{question}" -- if the question cannot be answered using the text,'
        " simply summarize the text. "
        "Include all factual information, numbers, stats etc if available.",
    }

def write_to_file(filename: str, text: str) -> None:
    """Write text to a file

    Args:
        text (str): The text to write
        filename (str): The filename to write to
    """
    with open(filename, "w") as file:
        file.write(text)

async def write_md_to_pdf(task: str, path: str, text: str) -> None:
    file_path = f"{path}/{task}"
    write_to_file(f"{file_path}.md", text)
    md_to_pdf(f"{file_path}.md", f"{file_path}.pdf")
    print(f"{task} written to {file_path}.pdf")

    encoded_file_path = urllib.parse.quote(f"{file_path}.pdf")

    return encoded_file_path

def read_txt_files(directory):
    all_text = ''

    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r') as file:
                all_text += file.read() + '\n'

    return all_text


def md_to_pdf(input_file, output_file):
    md2pdf(output_file,
           md_content=None,
           md_file_path=input_file,
           css_file_path=None,
           base_url=None)


==============================

-web_scrape.py
"""Selenium web scraping module."""
from __future__ import annotations

import logging
import asyncio
from pathlib import Path
from sys import platform

from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.firefox import GeckoDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from fastapi import WebSocket

import processing.text as summary

from config import Config
from processing.html import extract_hyperlinks, format_hyperlinks

from concurrent.futures import ThreadPoolExecutor


executor = ThreadPoolExecutor()

FILE_DIR = Path(__file__).parent.parent
CFG = Config()


async def async_browse(url: str, question: str, websocket: WebSocket) -> str:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user
        websocket (WebSocketManager): The websocket manager

    Returns:
        str: The answer and links to the user
    """
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=8)

    print(f"WEB_SCRAPE Scraping url {url} with question {question}")
    await websocket.send_json(
        {
            "type": "logs",
            "output": f"ðŸ”Ž Browsing the {url} for relevant about: {question}...",
        }
    )

    try:
        driver, text = await loop.run_in_executor(
            executor, scrape_text_with_selenium, url
        )
        await loop.run_in_executor(executor, add_header, driver)
        summary_text = await loop.run_in_executor(
            executor, summary.summarize_text, url, text, question, driver
        )

        await websocket.send_json(
            {
                "type": "logs",
                "output": f"ðŸ“ Information gathered from url {url}: {summary_text}",
            }
        )

        return f"Information gathered from url {url}: {summary_text}"
    except Exception as e:
        print(f"An error occurred while processing the url {url}: {e}")
        return f"Error processing the url {url}: {e}"


def browse_website(url: str, question: str) -> tuple[str, WebDriver]:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user

    Returns:
        Tuple[str, WebDriver]: The answer and links to the user and the webdriver
    """

    if not url:
        return "A URL was not specified, cancelling request to browse website.", None

    driver, text = scrape_text_with_selenium(url)
    add_header(driver)
    summary_text = summary.summarize_text(url, text, question, driver)

    links = scrape_links_with_selenium(driver, url)

    # Limit links to 5
    if len(links) > 5:
        links = links[:5]

    # write_to_file('research-{0}.txt'.format(url), summary_text + "\nSource Links: {0}\n\n".format(links))

    close_browser(driver)
    return f"Answer gathered from website: {summary_text} \n \n Links: {links}", driver


def scrape_text_with_selenium(url: str) -> tuple[WebDriver, str]:
    """Scrape text from a website using selenium

    Args:
        url (str): The url of the website to scrape

    Returns:
        Tuple[WebDriver, str]: The webdriver and the text scraped from the website
    """
    logging.getLogger("selenium").setLevel(logging.CRITICAL)

    options_available = {
        "chrome": ChromeOptions,
        "safari": SafariOptions,
        "firefox": FirefoxOptions,
    }

    options = options_available[CFG.selenium_web_browser]()
    options.add_argument(f"user-agent={CFG.user_agent}")
    options.add_argument("--headless")
    options.add_argument("--enable-javascript")

    if CFG.selenium_web_browser == "firefox":
        service = Service(executable_path=GeckoDriverManager().install())
        driver = webdriver.Firefox(service=service, options=options)
    elif CFG.selenium_web_browser == "safari":
        # Requires a bit more setup on the users end
        # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari
        driver = webdriver.Safari(options=options)
    else:
        if platform == "linux" or platform == "linux2":
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--remote-debugging-port=9222")
        options.add_argument("--no-sandbox")
        options.add_experimental_option("prefs", {"download_restrictions": 3})
        driver = webdriver.Chrome(options=options)

    driver.get(url)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )

    # Get the HTML content directly from the browser's DOM
    page_source = driver.execute_script("return document.body.outerHTML;")
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    # text = soup.get_text()
    text = get_text(soup)

    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = "\n".join(chunk for chunk in chunks if chunk)

    # Print the scraped text
    print(f"Scraped content from {url}:\n{text}\n")

    return driver, text


def get_text(soup):
    """Get the text from the soup

    Args:
        soup (BeautifulSoup): The soup to get the text from

    Returns:
        str: The text from the soup
    """
    text = ""
    tags = ["h1", "h2", "h3", "h4", "h5", "p"]
    for element in soup.find_all(tags):  # Find all the <p> elements
        text += element.text + "\n\n"
    return text


def scrape_links_with_selenium(driver: WebDriver, url: str) -> list[str]:
    """Scrape links from a website using selenium

    Args:
        driver (WebDriver): The webdriver to use to scrape the links

    Returns:
        List[str]: The links scraped from the website
    """
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    hyperlinks = extract_hyperlinks(soup, url)

    # Limit hyperlinks to 5
    if len(hyperlinks) > 5:
        hyperlinks = hyperlinks[:5]

    return format_hyperlinks(hyperlinks)


def close_browser(driver: WebDriver) -> None:
    """Close the browser

    Args:
        driver (WebDriver): The webdriver to close

    Returns:
        None
    """
    driver.quit()


def add_header(driver: WebDriver) -> None:
    """Add a header to the website

    Args:
        driver (WebDriver): The webdriver to use to add the header

    Returns:
        None
    """
    driver.execute_script(open(f"{FILE_DIR}/js/overlay.js", "r").read())


==============================

-web_search.py
from __future__ import annotations
from googleapiclient.discovery import build
import json

def web_search(query: str, num_results: int = 3) -> str: # int - number of results to include with 1 search
    """Useful for general internet search queries using Google Custom Search."""

    print("WEB_SEARCH web_search started")  # Indicates the start of the function

    # Your Google Custom Search API Key and Search engine ID
    API_KEY = "AIzaSyAJxSc8T8_G6Ysxpg_W8cxwXndWu1tXjfs"
    SEARCH_ENGINE_ID = "9486572ea12954a0d"

    # Initialize the Google Custom Search API client
    service = build("customsearch", "v1", developerKey=API_KEY)
    search_results = []

    if not query:
        print("No query provided for web_search")  # Indicates an early return due to missing query
        return json.dumps(search_results)

    # Query Google Custom Search and limit the results
    print(f"WEB_SEARCH Executing web search for query: {query}")  # Indicates that a query is being made
    results = service.cse().list(q=query, cx=SEARCH_ENGINE_ID, num=num_results).execute()
    
    if not results or 'items' not in results:
        print("No results returned from the search query")  # Indicates a case where the search didn't return items
        return json.dumps(search_results)

    # Extract and return the search results
    for item in results['items']:
        search_results.append({
            "title": item.get("title", ""),
            "link": item.get("link", ""),
            "snippet": item.get("snippet", "")
        })

    print(f"WEB_SEARCH Returning {len(search_results)} search results")  # Indicates the function is about to conclude and return results
    return json.dumps(search_results, ensure_ascii=False, indent=4)

# You can add additional functions or utility methods below if needed


==============================

-writer.py
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from agent.prompts import generate_report_prompt, generate_agent_role_prompt
from config import Config

CFG = Config()

class WriterActor:
    def __init__(self):
        print("WRITER Initializing WriterActor...")
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", generate_agent_role_prompt(agent="Default Agent")),
            ("user", generate_report_prompt(question="{query}", research_summary="{results}"))
        ])

    @property
    def runnable(self):
        return {
            "answer": {
                "query": lambda x: x["query"],
                "results": lambda x: "\n\n".join(x["results"])
              } | self.prompt | self.model | StrOutputParser()
        }


==============================

-web_scrape.py
"""Selenium web scraping module."""
from __future__ import annotations

import logging
import asyncio
from pathlib import Path
from sys import platform

from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.firefox import GeckoDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from fastapi import WebSocket

import processing.text as summary

from config import Config
from processing.html import extract_hyperlinks, format_hyperlinks

from concurrent.futures import ThreadPoolExecutor


executor = ThreadPoolExecutor()

FILE_DIR = Path(__file__).parent.parent
CFG = Config()


async def async_browse(url: str, question: str, websocket: WebSocket) -> str:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user
        websocket (WebSocketManager): The websocket manager

    Returns:
        str: The answer and links to the user
    """
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=8)

    print(f"WEB_SCRAPE Scraping url {url} with question {question}")
    await websocket.send_json(
        {
            "type": "logs",
            "output": f"ðŸ”Ž Browsing the {url} for relevant about: {question}...",
        }
    )

    try:
        driver, text = await loop.run_in_executor(
            executor, scrape_text_with_selenium, url
        )
        await loop.run_in_executor(executor, add_header, driver)
        summary_text = await loop.run_in_executor(
            executor, summary.summarize_text, url, text, question, driver
        )

        await websocket.send_json(
            {
                "type": "logs",
                "output": f"ðŸ“ Information gathered from url {url}: {summary_text}",
            }
        )

        return f"Information gathered from url {url}: {summary_text}"
    except Exception as e:
        print(f"An error occurred while processing the url {url}: {e}")
        return f"Error processing the url {url}: {e}"


def browse_website(url: str, question: str) -> tuple[str, WebDriver]:
    """Browse a website and return the answer and links to the user

    Args:
        url (str): The url of the website to browse
        question (str): The question asked by the user

    Returns:
        Tuple[str, WebDriver]: The answer and links to the user and the webdriver
    """

    if not url:
        return "A URL was not specified, cancelling request to browse website.", None

    driver, text = scrape_text_with_selenium(url)
    add_header(driver)
    summary_text = summary.summarize_text(url, text, question, driver)

    links = scrape_links_with_selenium(driver, url)

    # Limit links to 5
    if len(links) > 5:
        links = links[:5]

    # write_to_file('research-{0}.txt'.format(url), summary_text + "\nSource Links: {0}\n\n".format(links))

    close_browser(driver)
    return f"Answer gathered from website: {summary_text} \n \n Links: {links}", driver


def scrape_text_with_selenium(url: str) -> tuple[WebDriver, str]:
    """Scrape text from a website using selenium

    Args:
        url (str): The url of the website to scrape

    Returns:
        Tuple[WebDriver, str]: The webdriver and the text scraped from the website
    """
    logging.getLogger("selenium").setLevel(logging.CRITICAL)

    options_available = {
        "chrome": ChromeOptions,
        "safari": SafariOptions,
        "firefox": FirefoxOptions,
    }

    options = options_available[CFG.selenium_web_browser]()
    options.add_argument(f"user-agent={CFG.user_agent}")
    options.add_argument("--headless")
    options.add_argument("--enable-javascript")

    if CFG.selenium_web_browser == "firefox":
        service = Service(executable_path=GeckoDriverManager().install())
        driver = webdriver.Firefox(service=service, options=options)
    elif CFG.selenium_web_browser == "safari":
        # Requires a bit more setup on the users end
        # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari
        driver = webdriver.Safari(options=options)
    else:
        if platform == "linux" or platform == "linux2":
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--remote-debugging-port=9222")
        options.add_argument("--no-sandbox")
        options.add_experimental_option("prefs", {"download_restrictions": 3})
        driver = webdriver.Chrome(options=options)

    driver.get(url)

    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )

    # Get the HTML content directly from the browser's DOM
    page_source = driver.execute_script("return document.body.outerHTML;")
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    # text = soup.get_text()
    text = get_text(soup)

    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = "\n".join(chunk for chunk in chunks if chunk)

    # Print the scraped text
    print(f"Scraped content from {url}:\n{text}\n")

    return driver, text


def get_text(soup):
    """Get the text from the soup

    Args:
        soup (BeautifulSoup): The soup to get the text from

    Returns:
        str: The text from the soup
    """
    text = ""
    tags = ["h1", "h2", "h3", "h4", "h5", "p"]
    for element in soup.find_all(tags):  # Find all the <p> elements
        text += element.text + "\n\n"
    return text


def scrape_links_with_selenium(driver: WebDriver, url: str) -> list[str]:
    """Scrape links from a website using selenium

    Args:
        driver (WebDriver): The webdriver to use to scrape the links

    Returns:
        List[str]: The links scraped from the website
    """
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, "html.parser")

    for script in soup(["script", "style"]):
        script.extract()

    hyperlinks = extract_hyperlinks(soup, url)

    # Limit hyperlinks to 5
    if len(hyperlinks) > 5:
        hyperlinks = hyperlinks[:5]

    return format_hyperlinks(hyperlinks)


def close_browser(driver: WebDriver) -> None:
    """Close the browser

    Args:
        driver (WebDriver): The webdriver to close

    Returns:
        None
    """
    driver.quit()


def add_header(driver: WebDriver) -> None:
    """Add a header to the website

    Args:
        driver (WebDriver): The webdriver to use to add the header

    Returns:
        None
    """
    driver.execute_script(open(f"{FILE_DIR}/js/overlay.js", "r").read())


==============================

-web_search.py
from __future__ import annotations
from googleapiclient.discovery import build
import json

def web_search(query: str, num_results: int = 3) -> str: # int - number of results to include with 1 search
    """Useful for general internet search queries using Google Custom Search."""

    print("WEB_SEARCH web_search started")  # Indicates the start of the function

    # Your Google Custom Search API Key and Search engine ID
    API_KEY = "AIzaSyAJxSc8T8_G6Ysxpg_W8cxwXndWu1tXjfs"
    SEARCH_ENGINE_ID = "9486572ea12954a0d"

    # Initialize the Google Custom Search API client
    service = build("customsearch", "v1", developerKey=API_KEY)
    search_results = []

    if not query:
        print("No query provided for web_search")  # Indicates an early return due to missing query
        return json.dumps(search_results)

    # Query Google Custom Search and limit the results
    print(f"WEB_SEARCH Executing web search for query: {query}")  # Indicates that a query is being made
    results = service.cse().list(q=query, cx=SEARCH_ENGINE_ID, num=num_results).execute()
    
    if not results or 'items' not in results:
        print("No results returned from the search query")  # Indicates a case where the search didn't return items
        return json.dumps(search_results)

    # Extract and return the search results
    for item in results['items']:
        search_results.append({
            "title": item.get("title", ""),
            "link": item.get("link", ""),
            "snippet": item.get("snippet", "")
        })

    print(f"WEB_SEARCH Returning {len(search_results)} search results")  # Indicates the function is about to conclude and return results
    return json.dumps(search_results, ensure_ascii=False, indent=4)

# You can add additional functions or utility methods below if needed


==============================

-llm_utils.py
from __future__ import annotations

import json

from fastapi import WebSocket
import time

import openai
from langchain.adapters import openai as lc_openai
from colorama import Fore, Style
from openai.error import APIError, RateLimitError

from agent.prompts import auto_agent_instructions
from config import Config

CFG = Config()

openai.api_key = CFG.openai_api_key

from typing import Optional
import logging


def create_chat_completion(
    messages: list,  # type: ignore
    model: Optional[str] = None,
    temperature: float = CFG.temperature,
    max_tokens: Optional[int] = None,
    stream: Optional[bool] = False,
    websocket: WebSocket | None = None,
) -> str:
    """Create a chat completion using the OpenAI API
    Args:
        messages (list[dict[str, str]]): The messages to send to the chat completion
        model (str, optional): The model to use. Defaults to None.
        temperature (float, optional): The temperature to use. Defaults to 0.9.
        max_tokens (int, optional): The max tokens to use. Defaults to None.
        stream (bool, optional): Whether to stream the response. Defaults to False.
    Returns:
        str: The response from the chat completion
    """

    # validate input
    if model is None:
        raise ValueError("Model cannot be None")
    if max_tokens is not None and max_tokens > 8001:
        raise ValueError(f"Max tokens cannot be more than 8001, but got {max_tokens}")
    if stream and websocket is None:
        raise ValueError("Websocket cannot be None when stream is True")

    # create response
    for attempt in range(10):  # maximum of 10 attempts
        response = send_chat_completion_request(
            messages, model, temperature, max_tokens, stream, websocket
        )
        return response

    logging.error("Failed to get response from OpenAI API")
    raise RuntimeError("Failed to get response from OpenAI API")


def send_chat_completion_request(
    messages, model, temperature, max_tokens, stream, websocket
):
    if not stream:
        result = lc_openai.ChatCompletion.create(
            model=model,  # Change model here to use different models
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            provider=CFG.llm_provider,  # Change provider here to use a different API
        )
        return result["choices"][0]["message"]["content"]
    else:
        return stream_response(model, messages, temperature, max_tokens, websocket)


async def stream_response(model, messages, temperature, max_tokens, websocket):
    paragraph = ""
    response = ""
    print(f"LLM_UTILS streaming response...")

    for chunk in lc_openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        provider=CFG.llm_provider,
        stream=True,
    ):
        content = chunk["choices"][0].get("delta", {}).get("content")
        if content is not None:
            response += content
            paragraph += content
            if "\n" in paragraph:
                await websocket.send_json({"type": "report", "output": paragraph})
                paragraph = ""
    print(f"LLM_UTILS streaming response complete")
    return response


def choose_agent(task: str) -> dict:
    """Determines what agent should be used
    Args:
        task (str): The research question the user asked
    Returns:
        agent - The agent that will be used
        agent_role_prompt (str): The prompt for the agent
    """
    try:
        response = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{auto_agent_instructions()}"},
                {"role": "user", "content": f"task: {task}"},
            ],
            temperature=0,
        )

        return json.loads(response)
    except Exception as e:
        print(f"{Fore.RED}Error in choose_agent: {e}{Style.RESET_ALL}")
        return {
            "agent": "Default Agent",
            "agent_role_prompt": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
        }


==============================

-prompts.py
from datetime import datetime


def generate_agent_role_prompt(agent):
    """Generates the agent role prompt.
    Args: agent (str): The type of the agent.
    Returns: str: The agent role prompt.
    """
    prompts = {
        "Finance Agent": "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.",
        "Travel Agent": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.",
        "Academic Research Agent": "You are an AI academic research assistant. Your primary responsibility is to create thorough, academically rigorous, unbiased, and systematically organized reports on a given research topic, following the standards of scholarly work.",
        "Business Analyst": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.",
        "Computer Security Analyst Agent": "You are an AI specializing in computer security analysis. Your principal duty is to generate comprehensive, meticulously detailed, impartial, and systematically structured reports on computer security topics. This includes Exploits, Techniques, Threat Actors, and Advanced Persistent Threat (APT) Groups. All produced reports should adhere to the highest standards of scholarly work and provide in-depth insights into the complexities of computer security.",
        "Default Agent": "You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.",
    }

    return prompts.get(agent, "No such agent")


def generate_report_prompt(question, research_summary):
    """Generates the report prompt for the given question and research summary.
    Args: question (str): The question to generate the report prompt for
            research_summary (str): The research summary to generate the report prompt for
    Returns: str: The report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, answer the following'
        f' question or topic: "{question}" in a detailed report --'
        " The report should focus on the answer to the question, should be well structured, informative,"
        " in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format.\n "
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n"
        f"Write all used source urls at the end of the report in apa format.\n "
        f"Assume that the current date is {datetime.now().strftime('%B %d, %Y')}"
    )


def generate_search_queries_prompt(question):
    """Generates the search queries prompt for the given question.
    Args: question (str): The question to generate the search queries prompt for
    Returns: str: The search queries prompt for the given question
    """
    print(
        f"PROMPTS Generating search queries prompt for: {question}"
    )  # Indicates the specific question being processed for search queries

    queries = (
        f'Write 3 google search queries to search online that form an objective opinion from the following: "{question}"'
        f'Use the current date if needed: {datetime.now().strftime("%B %d, %Y")}.\n'
        f'You must respond with a list of strings in the following format: ["query 1", "query 2", "query 3"].'
    )
    print(f"PROMPTS Generated search queries: {queries}")  # Print the queries

    return queries


def generate_resource_report_prompt(question, research_summary):
    """Generates the resource report prompt for the given question and research summary.

    Args:
        question (str): The question to generate the resource report prompt for.
        research_summary (str): The research summary to generate the resource report prompt for.

    Returns:
        str: The resource report prompt for the given question and research summary.
    """
    return (
        f'"""{research_summary}""" Based on the above information, generate a bibliography recommendation report for the following'
        f' question or topic: "{question}". The report should provide a detailed analysis of each recommended resource,'
        " explaining how each source can contribute to finding answers to the research question."
        " Focus on the relevance, reliability, and significance of each source."
        " Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax."
        " Include relevant facts, figures, and numbers whenever available."
        " The report should have a minimum length of 1,200 words."
    )


def generate_outline_report_prompt(question, research_summary):
    """Generates the outline report prompt for the given question and research summary.
    Args: question (str): The question to generate the outline report prompt for
            research_summary (str): The research summary to generate the outline report prompt for
    Returns: str: The outline report prompt for the given question and research summary
    """

    return (
        f'"""{research_summary}""" Using the above information, generate an outline for a research report in Markdown syntax'
        f' for the following question or topic: "{question}". The outline should provide a well-structured framework'
        " for the research report, including the main sections, subsections, and key points to be covered."
        " The research report should be detailed, informative, in-depth, and a minimum of 1,200 words."
        " Use appropriate Markdown syntax to format the outline and ensure readability."
    )


def generate_concepts_prompt(question, research_summary):
    """Generates the concepts prompt for the given question.
    Args: question (str): The question to generate the concepts prompt for
            research_summary (str): The research summary to generate the concepts prompt for
    Returns: str: The concepts prompt for the given question
    """

    return (
        f'"""{research_summary}""" Using the above information, generate a list of 5 main concepts to learn for a research report'
        f' on the following question or topic: "{question}". The outline should provide a well-structured framework'
        'You must respond with a list of strings in the following format: ["concepts 1", "concepts 2", "concepts 3", "concepts 4, concepts 5"]'
    )


def generate_lesson_prompt(concept):
    """
    Generates the lesson prompt for the given question.
    Args:
        concept (str): The concept to generate the lesson prompt for.
    Returns:
        str: The lesson prompt for the given concept.
    """

    prompt = (
        f"generate a comprehensive lesson about {concept} in Markdown syntax. This should include the definition"
        f"of {concept}, its historical background and development, its applications or uses in different"
        f"fields, and notable events or facts related to {concept}."
    )

    return prompt


def get_report_by_type(report_type):
    report_type_mapping = {
        "research_report": generate_report_prompt,
        "resource_report": generate_resource_report_prompt,
        "outline_report": generate_outline_report_prompt,
    }
    return report_type_mapping[report_type]


def auto_agent_instructions():
    return """
        This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific agent, defined by its type and role, with each agent requiring distinct instructions.
        Agent
        The agent is determined by the field of the topic and the specific name of the agent that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each agent type is associated with a corresponding emoji.

        examples:
        task: "should I invest in apple stocks?"
        response: 
        {
            "agent": "ðŸ’° Finance Agent",
            "agent_role_prompt: "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends."
        }
        task: "could reselling sneakers become profitable?"
        response: 
        { 
            "agent":  "ðŸ“ˆ Business Analyst Agent",
            "agent_role_prompt": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis."
        }
        task: "what are the most interesting sites in Tel Aviv?"
        response:
        {
            "agent:  "ðŸŒ Travel Agent",
            "agent_role_prompt": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights."
        }
    """


==============================

-research_agent.py
# Description: Research assistant class that handles the research process for a given question.

# libraries
import asyncio
import json
import hashlib
from actions.web_search import web_search
from actions.web_scrape import async_browse
from processing.text import (
    write_to_file,
    create_message,
    create_chat_completion,
    read_txt_files,
    write_md_to_pdf,
)
from config import Config
from agent import prompts
import os
import string

CFG = Config()


class ResearchAgent:
    def __init__(self, question, agent, agent_role_prompt, websocket=None):
        """Initializes the research assistant with the given question.
        Args: question (str): The question to research
        Returns: None
        """
        self.question = question
        self.agent = agent
        self.agent_role_prompt = (
            agent_role_prompt
            if agent_role_prompt
            else prompts.generate_agent_role_prompt(agent)
        )
        self.visited_urls = set()
        self.research_summary = ""
        self.dir_path = f"./outputs/{hashlib.sha1(question.encode()).hexdigest()}"
        self.websocket = websocket

        print("RESEARCH_AGENT ResearchAgent initialized with question:", self.question)

    async def stream_output(self, output):
        if not self.websocket:
            return print(output)
        await self.websocket.send_json({"type": "logs", "output": output})

    async def summarize(self, text, topic):
        """Summarizes the given text for the given topic.
        Args: text (str): The text to summarize
                topic (str): The topic to summarize the text for
        Returns: str: The summarized text
        """
        print("RESEARCH_AGENT Starting summarization for topic:", topic)

        messages = [create_message(text, topic)]
        await self.stream_output(f"ðŸ“ Summarizing text for query: {text}")

        return create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
        )

    async def get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """
        new_urls = []
        for url in url_set_input:
            if url not in self.visited_urls:
                await self.stream_output(f"âœ… Adding source url to research: {url}\n")

                self.visited_urls.add(url)
                new_urls.append(url)

        return new_urls

    async def call_agent(self, action, stream=False, websocket=None):
        messages = [
            {"role": "system", "content": self.agent_role_prompt},
            {
                "role": "user",
                "content": action,
            },
        ]
        answer = create_chat_completion(
            model=CFG.smart_llm_model,
            messages=messages,
            stream=stream,
            websocket=websocket,
        )
        return answer

    async def create_search_queries(self):
        """Modifies the method to use the user's input directly as the search query.
        Args: None
        Returns: list[str]: The search queries for the given question
        """
        # Directly use the user's input (self.question) as the search query
        await self.stream_output(
            f"ðŸ§  I will conduct my research based on the following query: '{self.question}'..."
        )
        return [self.question]

    async def async_search(self, query):
        """Runs the async search for the given query.
        Args: query (str): The query to run the async search for
        Returns: list[str]: The async search for the given query
        """
        search_results = json.loads(web_search(query))
        new_search_urls = await self.get_new_urls([url.get("link") for url in search_results])

        await self.stream_output(
            f"ðŸŒ Browsing the following sites for relevant information: {new_search_urls}..."
        )

        # Create a list to hold the coroutine objects
        tasks = [
            async_browse(url, query, self.websocket) for url in new_search_urls
        ]

        # Gather the results as they become available
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        return responses

    async def run_search_summary(self, query):
        """Runs the search summary for the given query.
        Args: query (str): The query to run the search summary for
        Returns: str: The search summary for the given query
        """
        await self.stream_output(f"ðŸ”Ž Running research for '{query}'...")

        responses = await self.async_search(query)

        result = "\n".join(responses)
        os.makedirs(
            os.path.dirname(f"{self.dir_path}/research-{query}.txt"), exist_ok=True
        )
        write_to_file(f"{self.dir_path}/research-{query}.txt", result)
        return result

    async def conduct_research(self):
        """Conducts the research for the given question.
        Args: None
        Returns: str: The research for the given question
        """
        self.research_summary = (
            read_txt_files(self.dir_path) if os.path.isdir(self.dir_path) else ""
        )

        if not self.research_summary:
            search_queries = await self.create_search_queries()
            for query in search_queries:
                research_result = await self.run_search_summary(query)
                self.research_summary += f"{research_result}\n\n"

        await self.stream_output(
            f"Total research words: {len(self.research_summary.split(' '))}"
        )

        return self.research_summary

    async def create_concepts(self):
        """Creates the concepts for the given question.
        Args: None
        Returns: list[str]: The concepts for the given question
        """
        result = await self.call_agent(
            prompts.generate_concepts_prompt(self.question, self.research_summary)
        )

        await self.stream_output(
            f"I will research based on the following concepts: {result}\n"
        )
        return json.loads(result)

    async def write_report(self, report_type, websocket=None):
        """Writes the report for the given question.
        Args: None
        Returns: str: The report for the given question
        """
        print("RESEARCH_AGENT Starting to write report of type:", report_type)

        report_type_func = prompts.get_report_by_type(report_type)
        await self.stream_output(
            f"âœï¸ Writing {report_type} for research task: {self.question}..."
        )

        answer = await self.call_agent(
            report_type_func(self.question, self.research_summary),
            stream=websocket is not None,
            websocket=websocket,
        )
        # if websocket is True than we are streaming gpt response, so we need to wait for the final response
        final_report = await answer if websocket else answer

        path = await write_md_to_pdf(report_type, self.dir_path, final_report)

        return answer, path

    async def write_lessons(self):
        """Writes lessons on essential concepts of the research.
        Args: None
        Returns: None
        """
        concepts = await self.create_concepts()
        for concept in concepts:
            answer = await self.call_agent(
                prompts.generate_lesson_prompt(concept), stream=True
            )
            await write_md_to_pdf("Lesson", self.dir_path, answer)


==============================

-run.py
#manage WebSocket connections and the execution of an agent's research task
import asyncio
import datetime

from typing import List, Dict
from fastapi import WebSocket
from config import check_openai_api_key
from agent.research_agent import ResearchAgent


class WebSocketManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}
        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}

    async def start_sender(self, websocket: WebSocket):
        queue = self.message_queues[websocket]
        while True:
            message = await queue.get()
            if websocket in self.active_connections:
                await websocket.send_text(message)
            else:
                break

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.message_queues[websocket] = asyncio.Queue()
        self.sender_tasks[websocket] = asyncio.create_task(self.start_sender(websocket))

    async def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        self.sender_tasks[websocket].cancel()
        del self.sender_tasks[websocket]
        del self.message_queues[websocket]

    async def start_streaming(self, task, report_type, agent, agent_role_prompt, websocket):
        report, path = await run_agent(task, report_type, agent, agent_role_prompt, websocket)
        return report, path


async def run_agent(task, report_type, agent, agent_role_prompt, websocket):
    check_openai_api_key()

    start_time = datetime.datetime.now()

    # await websocket.send_json({"type": "logs", "output": f"Start time: {str(start_time)}\n\n"})

    assistant = ResearchAgent(task, agent, agent_role_prompt, websocket)
    await assistant.conduct_research()

    report, path = await assistant.write_report(report_type, websocket)

    await websocket.send_json({"type": "path", "output": path})

    end_time = datetime.datetime.now()
    await websocket.send_json({"type": "logs", "output": f"\nEnd time: {end_time}\n"})
    await websocket.send_json({"type": "logs", "output": f"\nTotal run time: {end_time - start_time}\n"})

    return report, path


==============================

-scripts.js
const GPTResearcher = (() => {
    const init = () => {
      // Not sure, but I think it would be better to add event handlers here instead of in the HTML
      //document.getElementById("startResearch").addEventListener("click", startResearch);
      //document.getElementById("copyToClipboard").addEventListener("click", copyToClipboard);

      updateState("initial");
    }

    const startResearch = () => {
      document.getElementById("output").innerHTML = "";
      document.getElementById("reportContainer").innerHTML = "";
      updateState("in_progress")
  
      addAgentResponse({ output: "ðŸ¤” Thinking about research questions for the task..." });
  
      listenToSockEvents();
    };
  
    const listenToSockEvents = () => {
      const { protocol, host, pathname } = window.location;
      const ws_uri = `${protocol === 'https:' ? 'wss:' : 'ws:'}//${host}${pathname}ws`;
      const converter = new showdown.Converter();
      const socket = new WebSocket(ws_uri);
  
      socket.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.type === 'logs') {
          addAgentResponse(data);
        } else if (data.type === 'report') {
          writeReport(data, converter);
        } else if (data.type === 'path') {
          updateState("finished")
          updateDownloadLink(data);

        }
      };
  
      socket.onopen = (event) => {
        const task = document.querySelector('input[name="task"]').value;
        const report_type = document.querySelector('select[name="report_type"]').value;
        const agent = document.querySelector('input[name="agent"]:checked').value;
  
        const requestData = {
          task: task,
          report_type: report_type,
          agent: agent,
        };
  
        socket.send(`start ${JSON.stringify(requestData)}`);
      };
    };
  
    const addAgentResponse = (data) => {
      const output = document.getElementById("output");
      output.innerHTML += '<div class="agent_response">' + data.output + '</div>';
      output.scrollTop = output.scrollHeight;
      output.style.display = "block";
      updateScroll();
    };
  
    const writeReport = (data, converter) => {
      const reportContainer = document.getElementById("reportContainer");
      const markdownOutput = converter.makeHtml(data.output);
      reportContainer.innerHTML += markdownOutput;
      updateScroll();
    };
  
    const updateDownloadLink = (data) => {
      const path = data.output;
      document.getElementById("downloadLink").setAttribute("href", path);
    };
  
    const updateScroll = () => {
      window.scrollTo(0, document.body.scrollHeight);
    };
  
    const copyToClipboard = () => {
      const textarea = document.createElement('textarea');
      textarea.id = 'temp_element';
      textarea.style.height = 0;
      document.body.appendChild(textarea);
      textarea.value = document.getElementById('reportContainer').innerText;
      const selector = document.querySelector('#temp_element');
      selector.select();
      document.execCommand('copy');
      document.body.removeChild(textarea);
    };

    const updateState = (state) => {
      var status = "";
      switch (state) {
        case "in_progress":
          status = "Research in progress..."
          setReportActionsStatus("disabled");
          break;
        case "finished":
          status = "Research finished!"
          setReportActionsStatus("enabled");
          break;
        case "error":
          status = "Research failed!"
          setReportActionsStatus("disabled");
          break;
        case "initial":
          status = ""
          setReportActionsStatus("hidden");
          break;
        default:
          setReportActionsStatus("disabled");
      }
      document.getElementById("status").innerHTML = status;
      if (document.getElementById("status").innerHTML == "") {
        document.getElementById("status").style.display = "none";
      } else {
        document.getElementById("status").style.display = "block";
      }
    }

    /**
     * Shows or hides the download and copy buttons
     * @param {str} status Kind of hacky. Takes "enabled", "disabled", or "hidden". "Hidden is same as disabled but also hides the div"
     */
    const setReportActionsStatus = (status) => {
      const reportActions = document.getElementById("reportActions");
      // Disable everything in reportActions until research is finished

      if (status == "enabled") {
        reportActions.querySelectorAll("a").forEach((link) => {
          link.classList.remove("disabled");
          link.removeAttribute('onclick');
          reportActions.style.display = "block";
        });
      } else {
        reportActions.querySelectorAll("a").forEach((link) => {
          link.classList.add("disabled");
          link.setAttribute('onclick', "return false;");
        });
        if (status == "hidden") {
          reportActions.style.display = "none";
        }
      }
    }

    document.addEventListener("DOMContentLoaded", init);
    return {
      startResearch,
      copyToClipboard,
    };
  })();

==============================

-__init__.py
from config.config import Config, check_openai_api_key
from config.singleton import AbstractSingleton, Singleton

__all__ = [
    "check_openai_api_key",
    "AbstractSingleton",
    "Config",
    "Singleton",
]


==============================

-config.py
"""Configuration class to store the state of bools for different scripts access."""
import os

import openai
from colorama import Fore
from dotenv import load_dotenv

from config.singleton import Singleton

load_dotenv(verbose=True)


class Config(metaclass=Singleton):
    """
    Configuration class to store the state of bools for different scripts access.
    """

    def __init__(self) -> None:
        """Initialize the Config class"""
        self.debug_mode = False
        self.allow_downloads = False

        self.selenium_web_browser = os.getenv("USE_WEB_BROWSER", "chrome")
        self.llm_provider = os.getenv("LLM_PROVIDER", "ChatOpenAI")
        self.fast_llm_model = os.getenv("FAST_LLM_MODEL", "gpt-3.5-turbo-16k")
        self.smart_llm_model = os.getenv("SMART_LLM_MODEL", "gpt-4")
        self.fast_token_limit = int(os.getenv("FAST_TOKEN_LIMIT", 2000))
        self.smart_token_limit = int(os.getenv("SMART_TOKEN_LIMIT", 4000))
        self.browse_chunk_max_length = int(os.getenv("BROWSE_CHUNK_MAX_LENGTH", 8192))
        self.summary_token_limit = int(os.getenv("SUMMARY_TOKEN_LIMIT", 700))

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.temperature = float(os.getenv("TEMPERATURE", "1"))

        self.user_agent = os.getenv(
            "USER_AGENT",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36"
            " (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36",
        )

        self.memory_backend = os.getenv("MEMORY_BACKEND", "local")
        # Initialize the OpenAI API client
        openai.api_key = self.openai_api_key

    def set_fast_llm_model(self, value: str) -> None:
        """Set the fast LLM model value."""
        self.fast_llm_model = value

    def set_smart_llm_model(self, value: str) -> None:
        """Set the smart LLM model value."""
        self.smart_llm_model = value

    def set_fast_token_limit(self, value: int) -> None:
        """Set the fast token limit value."""
        self.fast_token_limit = value

    def set_smart_token_limit(self, value: int) -> None:
        """Set the smart token limit value."""
        self.smart_token_limit = value

    def set_browse_chunk_max_length(self, value: int) -> None:
        """Set the browse_website command chunk max length value."""
        self.browse_chunk_max_length = value

    def set_openai_api_key(self, value: str) -> None:
        """Set the OpenAI API key value."""
        self.openai_api_key = value

    def set_debug_mode(self, value: bool) -> None:
        """Set the debug mode value."""
        self.debug_mode = value


def check_openai_api_key() -> None:
    """Check if the OpenAI API key is set in config.py or as an environment variable."""
    cfg = Config()
    if not cfg.openai_api_key:
        print(
            Fore.RED
            + "Please set your OpenAI API key in .env or as an environment variable."
        )
        print("You can get your key from https://platform.openai.com/account/api-keys")
        exit(1)


==============================

-singleton.py
"""The singleton metaclass for ensuring only one instance of a class."""
import abc


class Singleton(abc.ABCMeta, type):
    """
    Singleton metaclass for ensuring only one instance of a class.
    """

    _instances = {}

    def __call__(cls, *args, **kwargs):
        """Call method for the singleton metaclass."""
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class AbstractSingleton(abc.ABC, metaclass=Singleton):
    """
    Abstract singleton class for ensuring only one instance of a class.
    """

    pass


==============================

-overlay.js
const overlay = document.createElement('div');
Object.assign(overlay.style, {
    position: 'fixed',
    zIndex: 999999,
    top: 0,
    left: 0,
    width: '100%',
    height: '100%',
    background: 'rgba(0, 0, 0, 0.7)',
    color: '#fff',
    fontSize: '24px',
    fontWeight: 'bold',
    display: 'flex',
    justifyContent: 'center',
    alignItems: 'center',
});
const textContent = document.createElement('div');
Object.assign(textContent.style, {
    textAlign: 'center',
});
textContent.textContent = 'Tavily AI: Analyzing Page';
overlay.appendChild(textContent);
document.body.append(overlay);
document.body.style.overflow = 'hidden';
let dotCount = 0;
setInterval(() => {
    textContent.textContent = 'Tavily AI: Analyzing Page' + '.'.repeat(dotCount);
    dotCount = (dotCount + 1) % 4;
}, 1000);


==============================

-main.py
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import os

from agent.llm_utils import choose_agent
from agent.run import WebSocketManager


class ResearchRequest(BaseModel):
    task: str
    report_type: str
    agent: str



app = FastAPI()
app.mount("/site", StaticFiles(directory="client"), name="site")
app.mount("/static", StaticFiles(directory="client/static"), name="static")
# Dynamic directory for outputs once first research is run
@app.on_event("startup")
def startup_event():
    if not os.path.isdir("outputs"):
        os.makedirs("outputs")
    app.mount("/outputs", StaticFiles(directory="outputs"), name="outputs")

templates = Jinja2Templates(directory="client")

manager = WebSocketManager()


@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse('index.html', {"request": request, "report": None})


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            if data.startswith("start"):
                json_data = json.loads(data[6:])
                task = json_data.get("task")
                report_type = json_data.get("report_type")
                agent = json_data.get("agent")
                # temporary so "normal agents" can still be used and not just auto generated, will be removed when we move to auto generated
                if agent == "Auto Agent":
                    agent_dict = choose_agent(task)
                    agent = agent_dict.get("agent")
                    agent_role_prompt = agent_dict.get("agent_role_prompt")
                else:
                    agent_role_prompt = None

                await websocket.send_json({"type": "logs", "output": f"Initiated an Agent: {agent}"})
                if task and report_type and agent:
                    await manager.start_streaming(task, report_type, agent, agent_role_prompt, websocket)
                else:
                    print("Error: not enough parameters provided.")

    except WebSocketDisconnect:
        await manager.disconnect(websocket)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)


==============================

-editor.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

EDIT_TEMPLATE = """You are an editor. \
You have been tasked with editing the following draft, which was written by a non-expert. \
Please accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision. \
Things you should be checking for:

- This draft MUST fully answer the original question
- This draft MUST be written in apa format

If not all of the above criteria are met, you should send appropriate revision notes.
"""


class EditorActor:
    def __init__(self):
        print("EDITOR Initializing EditorActor")  # Indicates the start of the EditorActor initialization

        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(EDIT_TEMPLATE) + "Draft:\n\n{draft}"
        self.functions = [
            {
                "name": "revise",
                "description": "Sends the draft for revision",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "notes": {
                            "type": "string",
                            "description": "The editor's notes to guide the revision.",
                        },
                    },
                },
            },
            {
                "name": "accept",
                "description": "Accepts the draft",
                "parameters": {
                    "type": "object",
                    "properties": {"ready": {"const": True}},
                },
            },
        ]

    @property
    def runnable(self):
        return (
            self.prompt | self.model.bind(functions=self.functions)
        )


==============================

-research_team.py
from operator import itemgetter
from langchain.runnables.openai_functions import OpenAIFunctionsRouter

from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic

"""
    This is the research team. 
    It is a group of autonomous agents that work together to answer a given question
    using a comprehensive research process that includes: 
        - Searching for relevant information across multiple sources
        - Extracting relevant information
        - Writing a well structured report
        - Validating the report
        - Revising the report
        - Repeat until the report is satisfactory
"""


class ResearchTeam:
    def __init__(self, research_actor, editor_actor, reviser_actor):
        self.research_actor_instance = research_actor
        self.editor_actor_instance = editor_actor
        self.revise_actor_instance = reviser_actor

    def run(self, query):
        print("ResearchTeam: Initialized with query:", query)  # Starting the research process

        # create topics
        editor_inbox = Topic("editor_inbox")
        reviser_inbox = Topic("reviser_inbox")
        print("ResearchTeam: Topics created - editor_inbox, reviser_inbox")

        research_chain = (
            # Listed in inputs
            Topic.IN.subscribe()
            | {"draft": lambda x: self.research_actor_instance.run(x["question"])}
            # The draft always goes to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Research chain initialized")

        editor_chain = (
            # Listen for events in the editor_inbox
            editor_inbox.subscribe()
            | self.editor_actor_instance.runnable
            # Depending on the output, different things should happen
            | OpenAIFunctionsRouter(
                {
                    # If revise is chosen, we send a push to the critique_inbox
                    "revise": (
                        {
                            "notes": itemgetter("notes"),
                            "draft": editor_inbox.current() | itemgetter("draft"),
                            "question": Topic.IN.current() | itemgetter("question"),
                        }
                        | reviser_inbox.publish()
                    ),
                    # If accepted, then we return
                    "accept": editor_inbox.current() | Topic.OUT.publish(),
                }
            )
        )
        print("ResearchTeam: Editor chain initialized")

        reviser_chain = (
            # Listen for events in the reviser's inbox
            reviser_inbox.subscribe()
            | self.revise_actor_instance.runnable
            # Publish to the editor inbox
            | editor_inbox.publish()
        )
        print("ResearchTeam: Reviser chain initialized")

        web_researcher = PubSub(
            research_chain,
            editor_chain,
            reviser_chain,
            connection=InMemoryPubSubConnection(),
        )
        print("ResearchTeam: PubSub initialized with all chains")

        res = web_researcher.invoke({"question": query})
        print("ResearchTeam: Invocation complete with result:", res)

        return res["draft"]


==============================

-researcher.py
from permchain.connection_inmemory import InMemoryPubSubConnection
from permchain.pubsub import PubSub
from permchain.topic import Topic


class Researcher:
    def __init__(self, search_actor, writer_actor):
        print("RESEARCHER Initializing Researcher...")
        self.search_actor_instance = search_actor
        self.writer_actor_instance = writer_actor

    def run(self, query):
        # The research inbox
        research_inbox = Topic("research")
        search_actor = (
            Topic.IN.subscribe()
            | {"query": lambda x: x, "results": self.search_actor_instance.runnable}
            | research_inbox.publish()
        )

        write_actor = (
            research_inbox.subscribe()
            | self.writer_actor_instance.runnable
            | Topic.OUT.publish()
        )

        researcher = PubSub(
            search_actor,
            write_actor,
            connection=InMemoryPubSubConnection(),
        )

        res = researcher.invoke(query)
        return res["answer"]


==============================

-reviser.py
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import SystemMessagePromptTemplate
from config import Config

CFG = Config()

class ReviserActor:
    def __init__(self):
        print("REVISER Initializing ReviserActor")  # Indicates the start of the EditorActor initialization

        
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = SystemMessagePromptTemplate.from_template(
            "You are an expert writer. "
            "You have been tasked by your editor with revising the following draft, which was written by a non-expert. "
            "You may follow the editor's notes or not, as you see fit."
        ) + "Draft:\n\n{draft}" + "Editor's notes:\n\n{notes}"

    @property
    def runnable(self):
        return {
            "draft": {
                "draft": lambda x: x["draft"],
                "notes": lambda x: x["notes"],
            } | self.prompt | self.model | StrOutputParser()
        }


==============================

-gpt_researcher.py
import json
# Import necessary modules for text processing, web scraping, and searching
from processing.text import summarize_text
from actions.web_scrape import scrape_text_with_selenium
from actions.web_search import web_search

# Import Langchain and related utilities for AI-driven chat and prompt management
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableMap, RunnableLambda
from langchain.schema.messages import SystemMessage
from agent.prompts import auto_agent_instructions, generate_search_queries_prompt
from config import Config

# Load global configurations
CFG = Config()

# [Initiation] Prepare the AI-driven message template for generating search queries
search_message = (generate_search_queries_prompt("{question}"))
SEARCH_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "{agent_prompt}"),
    ("user", search_message)
])

# Load instructions for automated agent behavior
AUTO_AGENT_INSTRUCTIONS = auto_agent_instructions()
CHOOSE_AGENT_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessage(content=AUTO_AGENT_INSTRUCTIONS),
    ("user", "task: {task}")
])

# [Content Retrieval and Summarization and Analysis] Define the process for scraping and summarizing text from a URL
scrape_and_summarize = {
    "question": lambda x: x["question"],
    "text": lambda x: scrape_text_with_selenium(x['url'])[1],
    "url": lambda x: x['url']
} | RunnableMap({
        "summary": lambda x: summarize_text(text=x["text"], question=x["question"], url=x["url"]),
        "url": lambda x: x['url']
}) | (lambda x: f"Source Url: {x['url']}\nSummary: {x['summary']}")

# Initialize a set to keep track of URLs that have already been seen to avoid duplicate content
seen_urls = set()

# [Web Search and Content Retrieval] Define the process for conducting multiple searches, avoiding duplicate URLs, and processing the results
multi_search = (
    lambda x: [
        {"url": url.get("href"), "question": x["question"]}
        for url in json.loads(web_search(query=x["question"], num_results=3))
        if not (url.get("href") in seen_urls or seen_urls.add(url.get("href")))
   ]
) | scrape_and_summarize.map() | (lambda x: "\n".join(x))

# Set up the search query and agent choice mechanisms using AI models
search_query = SEARCH_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads
choose_agent = CHOOSE_AGENT_PROMPT | ChatOpenAI(model=CFG.smart_llm_model) | StrOutputParser() | json.loads

# [Initiation] Define how to get search queries based on agent prompts
get_search_queries = {
    "question": lambda x: x,
    "agent_prompt": {"task": lambda x: x} | choose_agent | (lambda x: x["agent_role_prompt"])
} | search_query


class GPTResearcherActor:
    # [Compilation and Output] Define the complete runnable process for the GPT Researcher, compiling all steps
    @property
    def runnable(self):
        return (
            get_search_queries
            | (lambda x: [{"question": q} for q in x])
            | multi_search.map()
            | (lambda x: "\n\n".join(x))
        )


==============================

-search_api.py
from tavily import Client
import os
from langchain.schema.runnable import RunnableLambda


class TavilySearchActor:
    def __init__(self):
        self.api_key = os.environ["TAVILY_API_KEY"]

    @property
    def runnable(self):
        client = Client(self.api_key)
        return RunnableLambda(client.advanced_search) | {"results": lambda x: x["results"]}


==============================

-test.py
# main
import os, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from permchain_example.researcher import Researcher
from permchain_example.search_actors.search_api import TavilySearchActor
from permchain_example.editor_actors.editor import EditorActor
from permchain_example.reviser_actors.reviser import ReviserActor
from permchain_example.search_actors.gpt_researcher import GPTResearcherActor
from permchain_example.writer_actors.writer import WriterActor
from permchain_example.research_team import ResearchTeam
from processing.text import md_to_pdf



if __name__ == '__main__':
    output_path = "./output"
    if not os.path.exists(output_path):
        # If the directory does not exist, create it
        os.makedirs(output_path)

    stocks = ["NVDA"]

    for stock in stocks[:1]:
        query = f"is the stock {stock} a good buy?"
        researcher = Researcher(GPTResearcherActor(), WriterActor())
        research_team = ResearchTeam(researcher, EditorActor(), ReviserActor())

        draft = research_team.run(query)
        with open(f"{output_path}/{stock}.md", "w") as f:
            f.write(draft)
        md_to_pdf(f"{output_path}/{stock}.md", f"{output_path}/{stock}.pdf")

==============================

-writer.py
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from agent.prompts import generate_report_prompt, generate_agent_role_prompt
from config import Config

CFG = Config()

class WriterActor:
    def __init__(self):
        print("WRITER Initializing WriterActor...")
        self.model = ChatOpenAI(model=CFG.smart_llm_model)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", generate_agent_role_prompt(agent="Default Agent")),
            ("user", generate_report_prompt(question="{query}", research_summary="{results}"))
        ])

    @property
    def runnable(self):
        return {
            "answer": {
                "query": lambda x: x["query"],
                "results": lambda x: "\n\n".join(x["results"])
              } | self.prompt | self.model | StrOutputParser()
        }


==============================

-__init__.py


==============================

-html.py
"""HTML processing functions"""
from __future__ import annotations

from bs4 import BeautifulSoup
from requests.compat import urljoin


def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """
    return [
        (link.text, urljoin(base_url, link["href"]))
        for link in soup.find_all("a", href=True)
    ]


def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """
    return [f"{link_text} ({link_url})" for link_text, link_url in hyperlinks]


==============================

-text.py
"""Text processing functions"""
import urllib
from typing import Dict, Generator, Optional
import string

from selenium.webdriver.remote.webdriver import WebDriver

from config import Config
from agent.llm_utils import create_chat_completion
import os
from md2pdf.core import md2pdf

CFG = Config()


def split_text(text: str, max_length: int = 8192) -> Generator[str, None, None]:
    """Split text into chunks of a maximum length

    Args:
        text (str): The text to split
        max_length (int, optional): The maximum length of each chunk. Defaults to 8192.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: If the text is longer than the maximum length
    """
    paragraphs = text.split("\n")
    current_length = 0
    current_chunk = []

    for paragraph in paragraphs:
        if current_length + len(paragraph) + 1 <= max_length:
            current_chunk.append(paragraph)
            current_length += len(paragraph) + 1
        else:
            yield "\n".join(current_chunk)
            current_chunk = [paragraph]
            current_length = len(paragraph) + 1

    if current_chunk:
        yield "\n".join(current_chunk)


def summarize_text(
    url: str, text: str, question: str, driver: Optional[WebDriver] = None
) -> str:
    """Summarize text using the OpenAI API

    Args:
        url (str): The url of the text
        text (str): The text to summarize
        question (str): The question to ask the model
        driver (WebDriver): The webdriver to use to scroll the page

    Returns:
        str: The summary of the text
    """
    if not text:
        return "Error: No text to summarize"

    summaries = []
    chunks = list(split_text(text))
    scroll_ratio = 1 / len(chunks)

    print(f"Summarizing url: {url} with total chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks):
        if driver:
            scroll_to_percentage(driver, scroll_ratio * i)

        #memory_to_add = f"Source: {url}\n" f"Raw content part#{i + 1}: {chunk}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

        messages = [create_message(chunk, question)]

        summary = create_chat_completion(
            model=CFG.fast_llm_model,
            messages=messages,
            max_tokens=CFG.summary_token_limit
        )
        summaries.append(summary)
        #memory_to_add = f"Source: {url}\n" f"Content summary part#{i + 1}: {summary}"

        #MEMORY.add_documents([Document(page_content=memory_to_add)])

    combined_summary = "\n".join(summaries)
    messages = [create_message(combined_summary, question)]

    final_summary = create_chat_completion(
        model=CFG.fast_llm_model,
        messages=messages,
        max_tokens=CFG.summary_token_limit
    )
    print("Final summary length: ", len(combined_summary))
    print(final_summary)

    return final_summary


def scroll_to_percentage(driver: WebDriver, ratio: float) -> None:
    """Scroll to a percentage of the page

    Args:
        driver (WebDriver): The webdriver to use
        ratio (float): The percentage to scroll to

    Raises:
        ValueError: If the ratio is not between 0 and 1
    """
    if ratio < 0 or ratio > 1:
        raise ValueError("Percentage should be between 0 and 1")
    driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {ratio});")


def create_message(chunk: str, question: str) -> Dict[str, str]:
    """Create a message for the chat completion

    Args:
        chunk (str): The chunk of text to summarize
        question (str): The question to answer

    Returns:
        Dict[str, str]: The message to send to the chat completion
    """
    return {
        "role": "user",
        "content": f'"""{chunk}""" Using the above text, answer in short the following'
        f' question: "{question}" -- if the question cannot be answered using the text,'
        " simply summarize the text. "
        "Include all factual information, numbers, stats etc if available.",
    }

def write_to_file(filename: str, text: str) -> None:
    """Write text to a file

    Args:
        text (str): The text to write
        filename (str): The filename to write to
    """
    with open(filename, "w") as file:
        file.write(text)

async def write_md_to_pdf(task: str, path: str, text: str) -> None:
    file_path = f"{path}/{task}"
    write_to_file(f"{file_path}.md", text)
    md_to_pdf(f"{file_path}.md", f"{file_path}.pdf")
    print(f"{task} written to {file_path}.pdf")

    encoded_file_path = urllib.parse.quote(f"{file_path}.pdf")

    return encoded_file_path

def read_txt_files(directory):
    all_text = ''

    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            with open(os.path.join(directory, filename), 'r') as file:
                all_text += file.read() + '\n'

    return all_text


def md_to_pdf(input_file, output_file):
    md2pdf(output_file,
           md_content=None,
           md_file_path=input_file,
           css_file_path=None,
           base_url=None)


==============================

-requirements.txt
# dependencies
asyncio==3.4.3
beautifulsoup4==4.12.2
colorama==0.4.6
duckduckgo_search==3.9.3
md2pdf==1.0.1
openai~=0.28.1
playwright==1.38.0
python-dotenv~=1.0.0
pyyaml==6.0.1
selenium
webdriver-manager==4.0.1
flask
uvicorn
pydantic
fastapi
python-multipart
markdown
langchain==0.0.308
tavily-python
permchain==0.0.3


==============================

-structure.py

